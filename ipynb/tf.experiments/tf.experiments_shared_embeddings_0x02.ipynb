{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Transformer (Base) Implementation (shared emb 0x02)\n",
    "----\n",
    "\n",
    "Well, I tried to understand the transformers in full detail but without \"using\" them in practice, but in theory. In this notebook I will simply reimplement/recreate the Tensorflow example transformer (https://www.tensorflow.org/tutorials/text/transformer) and later try to train it on my data, rather than to understand it first and then implement the transformer by myself from scratch (There is no value in doing so and it makes things even harder). \n",
    "\n",
    "I was way too much into the idea, to not use other peoples code and develop it on my own, but what I forgot is, that this ting is already invented and was implemented multiple times. When I started my project there weren't that many implementation out there, which were easy enough to understand, they were rather optimized for speed and did lots of tricks to achieve that. That made understanding the code so much harder, that I sticked to the idea to have to implement a transformer as simple as possible. But now thankfully such simple transformers exists (even as a tutorial). So there is no need to stick to this idea any further.\n",
    "\n",
    "I think it should be clear, that it is better to start with something that works (a fully implemented transformer) and then to experiment with it and then improve and modify it. As already stated, other transformer implementations were somehow too complicated and specialized, rather than simple. The Tensorflow transformer is simple enough to reimplement and to extend for own purposes.\n",
    "\n",
    "So the original code is licensed under Apache 2.0 License.\n",
    "\n",
    "`\n",
    "Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n",
    "`\n",
    "\n",
    "This code may still be rewritten and/or refactored later if it works. This python notebook should be seen as my way to start a transformer for my future experiments.\n",
    "\n",
    "Main of the code is not mine, but i made some modifications to it. For the original code please refer to the transformer tutorial website of tensorflow mentioned above.\n",
    "\n",
    "So here we go..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:21.841655Z",
     "start_time": "2020-10-17T08:45:17.723247Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the input pipeline\n",
    "----\n",
    "This is a translation example (NMT) from the method body into the method name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:23.011657Z",
     "start_time": "2020-10-17T08:45:22.996057Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T08:55:43.066232Z",
     "start_time": "2020-10-18T08:55:43.035032Z"
    }
   },
   "outputs": [],
   "source": [
    "# Length of a \"sentence\"\n",
    "MAX_LENGTH = 200\n",
    "\n",
    "# ??\n",
    "BUFFER_SIZE = 40000\n",
    "\n",
    "# Number of sentences processed in one batch\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T08:55:44.704235Z",
     "start_time": "2020-10-18T08:55:44.688635Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the context dataset\n",
    "context_dataset = tf.data.TextLineDataset( \n",
    "    os.path.join('D:\\\\Downloads\\\\Big-Code-excerpt','NextLineTranslationDataset.jsonl.from'))\n",
    "\n",
    "# load the prediction dataset\n",
    "nextline_dataset = tf.data.TextLineDataset(\n",
    "    os.path.join('D:\\\\Downloads\\\\Big-Code-excerpt','NextLineTranslationDataset.jsonl.to'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T08:55:58.932460Z",
     "start_time": "2020-10-18T08:55:58.916860Z"
    }
   },
   "outputs": [],
   "source": [
    "SYMBOL_START = 16273\n",
    "SYMBOL_EOS = 16274\n",
    "SYMBOL_PAD = 0\n",
    "VOCAB_SIZE = 16275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T08:56:01.709265Z",
     "start_time": "2020-10-18T08:56:00.180462Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# [16273] is the first element outside of the vocabulary, ans serves as a start element.\n",
    "# [0] serves as a padding element\n",
    "def my_json_decode(source, target):\n",
    "    source_decoded = [SYMBOL_START] + json.loads(source.numpy()) + [SYMBOL_EOS] + [SYMBOL_PAD]\n",
    "    target_decoded = [SYMBOL_START] + json.loads(target.numpy()) + [SYMBOL_EOS] + [SYMBOL_PAD]\n",
    "    return source_decoded, target_decoded\n",
    "\n",
    "def tf_context_nextline_json_decode(context, nextline):\n",
    "    result_context, result_nextline = tf.py_function(my_json_decode, [context, nextline], [tf.int64, tf.int64])\n",
    "    result_context.set_shape([None])\n",
    "    result_nextline.set_shape([None])\n",
    "\n",
    "    return result_context, result_nextline\n",
    "\n",
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)\n",
    "\n",
    "# combine both datasets in a parallel corpus\n",
    "train_dataset_ = tf.data.Dataset.zip((context_dataset, nextline_dataset))\n",
    "# transform from string to bpe encoded message\n",
    "train_dataset__ = train_dataset_.map(tf_context_nextline_json_decode)\n",
    "\n",
    "# filter dataset entries exceeding the capacity\n",
    "train_dataset = train_dataset__.filter(filter_max_length)\n",
    "\n",
    "# now do preprocessing and shuffle data around\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare distribution strategy\n",
    "\n",
    "We want to learn across multiple GPU's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:26.786863Z",
     "start_time": "2020-10-17T08:45:26.771263Z"
    }
   },
   "outputs": [],
   "source": [
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "#print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "----\n",
    "The positional encoding solves the problem that the transformer itself is not aware of the position of a word in a sentence, other than in RNNs or Convolutional networks. Therefore a positional encoding is added to the given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:26.802463Z",
     "start_time": "2020-10-17T08:45:26.786863Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:27.457665Z",
     "start_time": "2020-10-17T08:45:26.802463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50,512)\n",
    "print(pos_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:27.738465Z",
     "start_time": "2020-10-17T08:45:27.457665Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlim((0,512))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "----\n",
    "The masking is a feature which is required for the decoder side of the transformer architecture. If we want to detect the next word/token, we must not provide the \"solution\", but instead we have to cut the solution of while training the decoder. This is done via a upper diagonal mask. (also called look ahead mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:27.754065Z",
     "start_time": "2020-10-17T08:45:27.738465Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    # this will create a mask from the input, whereever the input is Zero, it is treated as a padding.\n",
    "    # and a one is written to the result, otherwise a Zero is written to the array (where true -> '1.0': else '0.0')\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "    # Mask has dimensions (batchsize, 1,1, seq_len)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:27.769665Z",
     "start_time": "2020-10-17T08:45:27.754065Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temp = create_look_ahead_mask(3)\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot product attention\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:27.785265Z",
     "start_time": "2020-10-17T08:45:27.769665Z"
    }
   },
   "outputs": [],
   "source": [
    "#def scaled_dot_product_attention(query, key, value, mask):\n",
    "#    matmul_qk = tf.matmul(query, key, transpose_b=True) \n",
    "#    \n",
    "#    embedding_dimensions = tf.cast(key.get_shape()[-1], tf.float32)\n",
    "#    scaled_attention_logits = matmul_qk / tf.math.sqrt(embedding_dimensions)\n",
    "#    \n",
    "#    if mask is not None:\n",
    "#        scaled_attention_logits += (mask * -1e9)\n",
    "#    \n",
    "#    # ..., seq_len_q, seq_len_k\n",
    "#    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "#    \n",
    "#    # ..., seq_len_q, depth_v\n",
    "#    output = tf.matmul(attention_weights, value)\n",
    "#    \n",
    "#    return output, attention_weights\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:28.112866Z",
     "start_time": "2020-10-17T08:45:28.081666Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "#y = tf.random.uniform((1,60,512))\n",
    "#out, attn = temp_mha(y,k=y, q=y, mask=None)\n",
    "#out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point wise feed forward network\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:28.612067Z",
     "start_time": "2020-10-17T08:45:28.596467Z"
    }
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#sample_ffn = point_wise_feed_forward_network(512,2048)\n",
    "#sample_ffn(tf.random.uniform((64,50,512))).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder / Decoder\n",
    "----\n",
    "Now the real fun part..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Layer\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:29.329668Z",
     "start_time": "2020-10-17T08:45:29.298468Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self,x,training,mask):\n",
    "        # first sublayer\n",
    "        attn_output,_ = self.mha(x,x,x,mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x+attn_output)\n",
    "        \n",
    "        # second sublayer\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample_encoder_layer = EncoderLayer(512,8,2048)\n",
    "sample_encoder_layer_output = sample_encoder_layer(tf.random.uniform((64,50,512)), False, None)\n",
    "sample_encoder_layer_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Layer\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:29.844469Z",
     "start_time": "2020-10-17T08:45:29.813269Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderLayer( tf.keras.layers.Layer ):\n",
    "    def __init__(self, d_model, num_heads, dff, rate = 0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # first sublayer\n",
    "        attn1, attn_weights_block1 = self.mha1(x,x,x,look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        \n",
    "        # second sublayer\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        \n",
    "        # third sublayer\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "        \n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample_decoder_layer = DecoderLayer(512,8,2048)\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(tf.random.uniform((64,50,512)), sample_encoder_layer_output, False, None, None)\n",
    "sample_decoder_layer_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:30.359270Z",
     "start_time": "2020-10-17T08:45:30.328070Z"
    }
   },
   "outputs": [],
   "source": [
    "common_embedding = None\n",
    "\n",
    "def get_common_embedding ( input_vocab_size, d_model ):\n",
    "    global common_embedding\n",
    "    \n",
    "    if common_embedding is None:\n",
    "        common_embedding =tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    \n",
    "    return common_embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:30.546470Z",
     "start_time": "2020-10-17T08:45:30.515270Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        #self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.embedding = get_common_embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "        \n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x*= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x+= self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        x= self.dropout(x, training = training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "                         input_vocab_size=8500,\n",
    "                         maximum_position_encoding=1000)\n",
    "\n",
    "temp_input=tf.random.uniform((64,62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "sample_encoder_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:31.061271Z",
     "start_time": "2020-10-17T08:45:31.045671Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = get_common_embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self,x,enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_length= tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "        # batchsize, sequence_length, d_model\n",
    "        x  =self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x +=self.pos_encoding[:, :seq_length, :]\n",
    "        \n",
    "        x= self.dropout(x, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x,  block1, block2 = self.dec_layers[i](x,enc_output, training, look_ahead_mask, padding_mask)\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "            \n",
    "        return x, attention_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "sample_decoder = Decoder(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    target_vocab_size = 8000, maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64,26), dtype=tf.int64,minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input, enc_output=sample_encoder_output, \n",
    "                              training=False, look_ahead_mask=None, padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Transformer\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:31.607272Z",
     "start_time": "2020-10-17T08:45:31.591672Z"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, \n",
    "                 input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "        \n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "        \n",
    "    def call(self, inp, tar, training, \n",
    "             enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        # batch_size, inp_seq_length, d_model\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        \n",
    "        final_output = self.final_layer(dec_output)\n",
    "        \n",
    "        return final_output, attention_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:31.794472Z",
     "start_time": "2020-10-17T08:45:31.778872Z"
    }
   },
   "outputs": [],
   "source": [
    "#sample_transformer = Transformer(\n",
    "#    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "#    input_vocab_size=8500, \n",
    "#    target_vocab_size=8000, \n",
    "#    pe_input=10000, pe_target=6000)\n",
    "\n",
    "#emp_input = tf.random.uniform((64,38), dtype=tf.int64, minval=0, maxval=200)\n",
    "#emp_target = tf.random.uniform((64,36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "#n_out, _ = sample_transformer(temp_input, temp_target, \n",
    "#                              training = False,\n",
    "#                              enc_padding_mask=None,\n",
    "#                              look_ahead_mask=None,\n",
    "#                              dec_padding_mask=None)\n",
    "\n",
    "#n_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#sample_transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The hyperparameters\n",
    "----\n",
    "Man sollte vermutlich erst einmal mit 2 oder 3 layern arbeiten. um zu sehen, ob das ganze funktioniert. Vielleicht spter mal mit 4 oder 6 Layern - Das groe Problem ist Rechenzeit...\n",
    "\n",
    "Einige der Parameter sind bereits fest. So zum beispiel, dass wir 512 dimensionale Vektoren verwenden. Diese sind bereits berechnet und sie waren aufwndig zu berechnen. Wir reden hier von mehreren Tagen mit meinem Setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:32.512073Z",
     "start_time": "2020-10-17T08:45:32.496473Z"
    }
   },
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 256\n",
    "dff=1024\n",
    "num_heads = 8\n",
    "\n",
    "bpe_encoder_vocab_size = 16272\n",
    "## +2 because of padding and masking\n",
    "input_vocab_size = bpe_encoder_vocab_size + 3\n",
    "target_vocab_size = bpe_encoder_vocab_size + 3\n",
    "\n",
    "\n",
    "#input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "#target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "\n",
    "\n",
    "dropout_rate = 0.09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer - The Custom Learning rate\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:32.917674Z",
     "start_time": "2020-10-17T08:45:32.902074Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model=d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        \n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5 )\n",
    "        \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:33.619675Z",
     "start_time": "2020-10-17T08:45:33.604075Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.995, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:33.962876Z",
     "start_time": "2020-10-17T08:45:33.775676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(sample_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Metrics\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:34.602477Z",
     "start_time": "2020-10-17T08:45:34.586877Z"
    }
   },
   "outputs": [],
   "source": [
    "#with strategy.scope():\n",
    "loss_object=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:34.774077Z",
     "start_time": "2020-10-17T08:45:34.758477Z"
    }
   },
   "outputs": [],
   "source": [
    "#with strategy.scope():\n",
    "def loss_function(real, pred):\n",
    "    mask=tf.math.logical_not(tf.math.equal(real,0))\n",
    "\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:34.992478Z",
     "start_time": "2020-10-17T08:45:34.945678Z"
    }
   },
   "outputs": [],
   "source": [
    "#with strategy.scope():\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Checkpointing\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:35.600879Z",
     "start_time": "2020-10-17T08:45:35.288878Z"
    }
   },
   "outputs": [],
   "source": [
    "# with strategy.scope():\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                      input_vocab_size, target_vocab_size,\n",
    "                     pe_input=input_vocab_size, pe_target=target_vocab_size,\n",
    "                     rate=dropout_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:35.616479Z",
     "start_time": "2020-10-17T08:45:35.600879Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    \n",
    "    # wird im second attentionblock im decoder benutzt, um den input zu maskieren\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "    \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "    \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:35.663279Z",
     "start_time": "2020-10-17T08:45:35.616479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./checkpoints/train_ce_20201003\"\n",
    "ckpt = tf.train.Checkpoint( transformer = transformer, optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:35.788079Z",
     "start_time": "2020-10-17T08:45:35.772479Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T08:45:35.959679Z",
     "start_time": "2020-10-17T08:45:35.944079Z"
    }
   },
   "outputs": [],
   "source": [
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None,None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None,None), dtype=tf.int64)\n",
    "]\n",
    "\n",
    "@tf.function(input_signature = train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    \n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    # Some zero gradients return null, we override them with zero\n",
    "    gradients = [gradient if gradient is not None else tf.zeros_like(var) for var, gradient in zip(transformer.trainable_variables, gradients)]\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T08:32:16.095761Z",
     "start_time": "2020-10-18T08:32:16.064561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.13535018364194"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decision_loss(train_loss, train_acuracy):\n",
    "    return train_loss / (1.000001-train_acuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T08:58:04.746681Z",
     "start_time": "2020-10-18T08:56:45.763742Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch129 Batch 0 Loss 3.1426 Accuracy 0.1550, Decision Loss 3.7193\n",
      "Epoch129 Batch 100 Loss 2.9919 Accuracy 0.1458, Decision Loss 3.5027\n",
      "Epoch129 Batch 200 Loss 2.9645 Accuracy 0.1422, Decision Loss 3.4560\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-c0189e1301d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\johndoe\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\johndoe\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\johndoe\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\johndoe\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\johndoe\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\johndoe\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\johndoe\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "                                         \n",
    "\n",
    "\n",
    "for epoch in range(128,130):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp,tar)\n",
    "\n",
    "        if batch%100 ==0:\n",
    "            print('Epoch{} Batch {} Loss {:.4f} Accuracy {:.4f}, Decision Loss {:.4f}'.format(\n",
    "                epoch+1, batch, train_loss.result(), train_accuracy.result(), decision_loss(train_loss.result(),train_accuracy.result() ))\n",
    "            )\n",
    "\n",
    "    if (epoch+1)%2 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print('saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "\n",
    "    print('Epoch {} Loss {:.4f} Accuracy {:.4f}, Decision Loss {:.4f}'.format(epoch+1, train_loss.result(), train_accuracy.result(),decision_loss(train_loss.result(),train_accuracy.result() )))\n",
    "\n",
    "    print('Time taken for one epoch: {} secs\\n'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the current Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T00:25:08.896620Z",
     "start_time": "2020-10-18T00:25:08.881020Z"
    }
   },
   "outputs": [],
   "source": [
    "model_save_path = '../../data/checkpoints/nextlineofcode_s_emb/v5/tf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T00:25:09.286620Z",
     "start_time": "2020-10-18T00:25:08.896620Z"
    }
   },
   "outputs": [],
   "source": [
    "transformer.save_weights(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T00:25:09.317820Z",
     "start_time": "2020-10-18T00:25:09.286620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Encoder)            multiple                  7325440   \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            multiple                  8380160   \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             multiple                  4182675   \n",
      "=================================================================\n",
      "Total params: 15,721,875\n",
      "Trainable params: 15,721,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the training using a validation dataset\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [tokenizer_pt.vocab_size]\n",
    "    end_token = [tokenizer_pt.vocab_size + 1]\n",
    "  \n",
    "    # inp sentence is portuguese, hence adding the start and end token\n",
    "    inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "  \n",
    "    # as the target is english, the first word to the transformer should be the\n",
    "    # english start token.\n",
    "    decoder_input = [tokenizer_en.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(MAX_LENGTH):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "  \n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "    \n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == tokenizer_en.vocab_size+1:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    sentence = tokenizer_pt.encode(sentence)\n",
    "\n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "\n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "\n",
    "        ax.set_ylim(len(result)-1.5, -0.5)\n",
    "\n",
    "        ax.set_xticklabels(\n",
    "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "            fontdict=fontdict, rotation=90)\n",
    "\n",
    "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                            if i < tokenizer_en.vocab_size], \n",
    "                           fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, plot=''):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "    predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                              if i<tokenizer_en.vocab_size])\n",
    "    \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted Translation: {}'.format(predicted_sentence))\n",
    "    \n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence, result, plot)\n",
    "        \n",
    "#translate(\"este  um problema que temos que resolver.\")\n",
    "#print (\"Real translation: this is a problem we have to solve .\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translate(\"este  o primeiro livro que eu fiz.\", plot='decoder_layer6_block2')\n",
    "#print (\"Real translation: this is the first book i've ever done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict first line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../../src')\n",
    "\n",
    "from com.github.c2nes.javalang import tokenizer as tokenizer\n",
    "\n",
    "from de.mindscan.fluentgenesis.bpe.bpe_model import BPEModel\n",
    "from de.mindscan.fluentgenesis.bpe.bpe_encoder_decoder import SimpleBPEEncoder\n",
    "\n",
    "from de.mindscan.fluentgenesis.transformer import TfTransformerV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_model = BPEModel(\"16K-full\", \"../../src/de/mindscan/fluentgenesis/bpe/\")\n",
    "bpe_model.load_hparams()\n",
    "\n",
    "bpe_model_vocabulary = bpe_model.load_tokens()\n",
    "bpe_model_bpe_data = bpe_model.load_bpe_pairs()\n",
    "\n",
    "bpe_encoder = SimpleBPEEncoder(bpe_model_vocabulary, bpe_model_bpe_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = 16273\n",
    "PAD_TOKEN = 0\n",
    "MAX_OUTPUTLENGTH = 64\n",
    "\n",
    "def tokenize_java_code(theSource: str):\n",
    "    tokens = list(tokenizer.tokenize(theSource, ignore_errors=True))\n",
    "    tokenvalues = [x.value for x in tokens]\n",
    "    \n",
    "    return tokenvalues\n",
    "\n",
    "def plot_prediction_attention_weights(attention, sentence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    " \n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    " \n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    " \n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    " \n",
    "        fontdict = {'fontsize': 10}\n",
    " \n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "        ax.set_yticks(range(len(result)+1))\n",
    " \n",
    "        ax.set_ylim(len(result)-1.5, -0.5)\n",
    " \n",
    "        ax.set_xticklabels(\n",
    "            ['<start>']+[bpe_encoder.decode([i])[0] for i in sentence]+['<end>'], \n",
    "            fontdict=fontdict, rotation=90)\n",
    " \n",
    "        ax.set_yticklabels([bpe_encoder.decode([i])[0] for i in result \n",
    "                            if i < START_TOKEN], \n",
    "                           fontdict=fontdict)\n",
    " \n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    " \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "# the following function shall sample the first line of a method, from a transformer\n",
    "# greedy decoder...\n",
    "#\n",
    "def sample_transformer_nextline(transformer, class_name, method_name, method_signature):\n",
    "    # we encode the class_name\n",
    "    # we encode the method signature\n",
    "    # we encode the line context\n",
    "    input_tokens = [SYMBOL_START] + bpe_encoder.encode([class_name,'.',method_name]) + bpe_encoder.encode( tokenize_java_code( method_signature)) + [SYMBOL_EOS]\n",
    "    encoderinput = tf.expand_dims(input_tokens,0)\n",
    "    \n",
    "    # this are the output tokens\n",
    "    output_tokens = []\n",
    "    # add start token to output_tokens\n",
    "    output_tokens.append(START_TOKEN)\n",
    "    output = tf.expand_dims(output_tokens,0)\n",
    "    \n",
    "    for _ in range(MAX_OUTPUTLENGTH):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoderinput,output)\n",
    "        \n",
    "        predictions, attention_weights = transformer(encoderinput,\n",
    "                                                     output, \n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask\n",
    "                                                     )\n",
    "        predictions = predictions[:,-1, :]\n",
    "        # greedy sampling\n",
    "        # predicted_id = tf.argmax(predictions, axis=-1, output_type=tf.int32)\n",
    "        predicted_id = tf.random.categorical(logits=predictions, num_samples=1)[0]\n",
    "        \n",
    "        if predicted_id in(PAD_TOKEN, SYMBOL_EOS):\n",
    "            return tf.squeeze(output, axis=0), attention_weights, input_tokens\n",
    "        \n",
    "        output = tf.concat( [output, [predicted_id]], axis=-1 )\n",
    "        \n",
    "    return tf.squeeze(output, axis=0), attention_weights, input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_first_line(transformer, class_name, method_name, method_signature, plot=''):\n",
    "    result, attention_weights, input_tokens = sample_transformer_nextline(transformer, class_name, method_name, method_signature)\n",
    "\n",
    "    #result = [i for i in result.numpy() if ((i > 0) and (i<16273))]\n",
    "    result = [i for i in result.numpy()]\n",
    "    input_tokens = [i for i in input_tokens if ((i > 0) and (i<16273))]\n",
    "    # input_tokens = [i for i in input_tokens]\n",
    "    \n",
    "    predicted_line = bpe_encoder.decode(result)\n",
    "    decoded_input_tokens = bpe_encoder.decode(input_tokens)\n",
    "    \n",
    "    print ('Input Context: {}'.format(decoded_input_tokens))\n",
    "    print ('Predicted output: {}'.format(predicted_line))\n",
    "    \n",
    "    if plot:\n",
    "        # plot the attention weights\n",
    "        plot_prediction_attention_weights(attention_weights, input_tokens, result, plot) \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_first_line(transformer, 'Config', 'getInstance', 'Config getInstance() if(config != null) { ', plot='decoder_layer4_block2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bpe_encoder.decode([701, 47, 1465, 701, 1465, 41, 42, 310, 41, 701, 47, 690, 412, 346, 42, 124])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer_restored = TfTransformerV1.Transformer(\n",
    "    num_layers=4, d_model=256, num_heads=8, dff=1024,\n",
    "    input_vocab_size=16274, target_vocab_size=16274,\n",
    "    pe_input=512, pe_target=512,\n",
    "    rate=0.0\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer_restored.load_weights(filepath='../../data/checkpoints/nextlineofcode_by_context/v3/tf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict_first_line(transformer_restored, 'Config', 'getInstance', 'Config getInstance() if (Config.instance == null) {')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
