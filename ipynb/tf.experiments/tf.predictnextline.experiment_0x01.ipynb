{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction using the pre-learned Transformer\n",
    "\n",
    "(C) Maxim Gansert, Mindscan Engineering, 2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../../src')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from com.github.c2nes.javalang import tokenizer as tokenizer\n",
    "\n",
    "from de.mindscan.fluentgenesis.bpe.bpe_model import BPEModel\n",
    "from de.mindscan.fluentgenesis.bpe.bpe_encoder_decoder import SimpleBPEEncoder\n",
    "\n",
    "from de.mindscan.fluentgenesis.transformer import TfTransformerV1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_model = BPEModel(\"16K-full\", \"../../src/de/mindscan/fluentgenesis/bpe/\")\n",
    "bpe_model.load_hparams()\n",
    "\n",
    "bpe_model_vocabulary = bpe_model.load_tokens()\n",
    "bpe_model_bpe_data = bpe_model.load_bpe_pairs()\n",
    "\n",
    "bpe_encoder = SimpleBPEEncoder(bpe_model_vocabulary, bpe_model_bpe_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    # this will create a mask from the input, whereever the input is Zero, it is treated as a padding.\n",
    "    # and a one is written to the result, otherwise a Zero is written to the array (where true -> '1.0': else '0.0')\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "    # Mask has dimensions (batchsize, 1,1, seq_len)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "    # encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    \n",
    "    # wird im second attentionblock im decoder benutzt, um den input zu maskieren\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "    \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "    \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = 16273\n",
    "PAD_TOKEN = 0\n",
    "MAX_OUTPUTLENGTH = 64\n",
    "\n",
    "def tokenize_java_code(theSource: str):\n",
    "    tokens = list(tokenizer.tokenize(theSource, ignore_errors=True))\n",
    "    tokenvalues = [x.value for x in tokens]\n",
    "    \n",
    "    return tokenvalues\n",
    "\n",
    "def plot_prediction_attention_weights(attention, sentence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    " \n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    " \n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    " \n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    " \n",
    "        fontdict = {'fontsize': 10}\n",
    " \n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    " \n",
    "        ax.set_ylim(len(result)-1.5, -0.5)\n",
    " \n",
    "        ax.set_xticklabels(\n",
    "            ['<start>']+[bpe_encoder.decode([i]) for i in sentence]+['<end>'], \n",
    "            fontdict=fontdict, rotation=90)\n",
    " \n",
    "        ax.set_yticklabels([bpe_encoder.decode([i]) for i in result \n",
    "                            if i < START_TOKEN], \n",
    "                           fontdict=fontdict)\n",
    " \n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    " \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "# the following function shall sample the first line of a method, from a transformer\n",
    "# greedy decoder...\n",
    "#\n",
    "def sample_transformer_nextline(transformer, class_name, method_name, method_signature):\n",
    "    # we encode the class_name\n",
    "    # we encode the method signature\n",
    "    # we encode the line context\n",
    "    input_tokens = [START_TOKEN] + bpe_encoder.encode([class_name,'.',method_name]) + bpe_encoder.encode( tokenize_java_code( method_signature)) + [PAD_TOKEN] + [PAD_TOKEN]\n",
    "    encoderinput = tf.expand_dims(input_tokens,0)\n",
    "    \n",
    "    # this are the output tokens\n",
    "    output_tokens = []\n",
    "    # add start token to output_tokens\n",
    "    output_tokens.append(START_TOKEN)\n",
    "    output = tf.expand_dims(output_tokens,0)\n",
    "    \n",
    "    for _ in range(MAX_OUTPUTLENGTH):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoderinput,output)\n",
    "        \n",
    "        predictions, attention_weights = transformer(encoderinput,\n",
    "                                                     output, \n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask\n",
    "                                                     )\n",
    "        predictions = predictions[:,-1, :]\n",
    "        # greedy sampling\n",
    "        predicted_id = tf.argmax(predictions, axis=-1, output_type=tf.int32)\n",
    "        \n",
    "        if predicted_id == PAD_TOKEN:\n",
    "            return tf.squeeze(output, axis=0), attention_weights, input_tokens\n",
    "        \n",
    "        output = tf.concat( [output, [predicted_id]], axis=-1 )\n",
    "        \n",
    "        # because of a lack of a good end of sentence symbol use \";\" or \"}\" as a trigger to detect \"end of sentence\"\n",
    "        # this is no good solution, but well, it is a solution right now, even with a messy dataset.\n",
    "        if predicted_id in (60,126):\n",
    "            return tf.squeeze(output, axis=0), attention_weights, input_tokens\n",
    "        \n",
    "    return tf.squeeze(output, axis=0), attention_weights, input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_first_line(transformer, class_name, method_name, method_signature, plot=''):\n",
    "    result, attention_weights, input_tokens = sample_transformer_nextline(transformer, class_name, method_name, method_signature)\n",
    "\n",
    "    result = [i for i in result.numpy() if ((i > 0) and (i<16273))]\n",
    "    input_tokens = [i for i in input_tokens if ((i > 0) and (i<16273))]\n",
    "    \n",
    "    predicted_line = bpe_encoder.decode(result)\n",
    "    decoded_input_tokens = bpe_encoder.decode(input_tokens)\n",
    "    \n",
    "    print ('Input Context: {}'.format(decoded_input_tokens))\n",
    "    print ('Predicted output: {}'.format(predicted_line))\n",
    "    \n",
    "    if plot:\n",
    "        # plot the attention weights\n",
    "        plot_prediction_attention_weights(attention_weights, input_tokens, result, plot) \n",
    "    \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_restored = TfTransformerV1.Transformer(\n",
    "    num_layers=4, d_model=256, num_heads=8, dff=1024,\n",
    "    input_vocab_size=16274, target_vocab_size=16274,\n",
    "    pe_input=512, pe_target=512,\n",
    "    rate=0.0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_restored.load_weights(filepath='../../data/checkpoints/nextlineofcode_by_context/v3/tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_first_line(transformer_restored, 'Config', 'getInstance', 'Config getInstance()')\n",
    "predict_first_line(transformer_restored, 'Config', 'getInstance', 'Config getInstance() if (Config.instance == null) {')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_encoder.encode([';', '}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
