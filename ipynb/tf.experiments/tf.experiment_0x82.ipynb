{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Transformer (Base) Implementation 0x80\n",
    "----\n",
    "\n",
    "Well, I tried to understand the transformers in full detail but without \"using\" them in practice, but in theory. In this notebook I will simply reimplement/recreate the Tensorflow example transformer (https://www.tensorflow.org/tutorials/text/transformer) and later try to train it on my data, rather than to understand it first and then implement the transformer by myself from scratch (There is no value in doing so and it makes things even harder). \n",
    "\n",
    "I was way too much into the idea, to not use other peoples code and develop it on my own, but what I forgot is, that this ting is already invented and was implemented multiple times. When I started my project there weren't that many implementation out there, which were easy enough to understand, they were rather optimized for speed and did lots of tricks to achieve that. That made understanding the code so much harder, that I sticked to the idea to have to implement a transformer as simple as possible. But now thankfully such simple transformers exists (even as a tutorial). So there is no need to stick to this idea any further.\n",
    "\n",
    "I think it should be clear, that it is better to start with something that works (a fully implemented transformer) and then to experiment with it and then improve and modify it. As already stated, other transformer implementations were somehow too complicated and specialized, rather than simple. The Tensorflow transformer is simple enough to reimplement and to extend for own purposes.\n",
    "\n",
    "So the original code is licensed under Apache 2.0 License.\n",
    "\n",
    "`\n",
    "Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n",
    "`\n",
    "\n",
    "This code may still be rewritten and/or refactored later if it works. This python notebook should be seen as my way to start a transformer for my future experiments.\n",
    "\n",
    "Main of the code is not mine, but i made some modifications to it. For the original code please refer to the transformer tutorial website of tensorflow mentioned above.\n",
    "\n",
    "So here we go..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the input pipeline\n",
    "----\n",
    "We follow this example for a protugese to english translation. So we prepare a well known/prepared dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of a \"sentence\"\n",
    "MAX_LENGTH = 64\n",
    "\n",
    "# ??\n",
    "BUFFER_SIZE = 50000\n",
    "\n",
    "# Number of sentences processed in one batch\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    \n",
    "    # load the context dataset\n",
    "    context_dataset = tf.data.TextLineDataset( \n",
    "        os.path.join('D:\\\\Downloads\\\\Big-Code-excerpt','NextLineTranslationDataset.jsonl.from'))\n",
    "\n",
    "    # load the prediction dataset\n",
    "    nextline_dataset = tf.data.TextLineDataset(\n",
    "        os.path.join('D:\\\\Downloads\\\\Big-Code-excerpt','NextLineTranslationDataset.jsonl.to'))\n",
    "\n",
    "    \n",
    "    # [16273] is the first element outside of the vocabulary, ans serves as a start element.\n",
    "    # [0] serves as a padding element\n",
    "    def my_json_decode(source, target):\n",
    "        source_decoded = [16273] + json.loads(source.numpy()) + [0] + [0]\n",
    "        target_decoded = [16273] + json.loads(target.numpy()) + [0] + [0]\n",
    "        return source_decoded, target_decoded\n",
    "\n",
    "    def tf_context_nextline_json_decode(context, nextline):\n",
    "        result_context, result_nextline = tf.py_function(my_json_decode, [context, nextline], [tf.int64, tf.int64])\n",
    "        result_context.set_shape([None])\n",
    "        result_nextline.set_shape([None])\n",
    "\n",
    "        return result_context, result_nextline\n",
    "\n",
    "    def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "        return tf.logical_and(tf.size(x) <= max_length,\n",
    "                            tf.size(y) <= max_length)\n",
    "\n",
    "    # combine both datasets in a parallel corpus\n",
    "    train_dataset_ = tf.data.Dataset.zip((context_dataset, nextline_dataset))\n",
    "    # transform from string to bpe encoded message\n",
    "    train_dataset__ = train_dataset_.map(tf_context_nextline_json_decode)\n",
    "\n",
    "    # filter dataset entries exceeding the capacity\n",
    "    train_dataset = train_dataset__.filter(filter_max_length)\n",
    "\n",
    "    # now do preprocessing and shuffle data around\n",
    "    train_dataset = train_dataset.cache()\n",
    "    train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "\n",
    "    def get_angles(pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def positional_encoding(position, d_model):\n",
    "        angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                              np.arange(d_model)[np.newaxis, :],\n",
    "                              d_model)\n",
    "\n",
    "        # apply sin to even indices in the array; 2i\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "        # apply cos to odd indices in the array; 2i+1\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "    pos_encoding = positional_encoding(50,512)\n",
    "    print(pos_encoding.shape)\n",
    "\n",
    "\n",
    "    def create_padding_mask(seq):\n",
    "        # this will create a mask from the input, whereever the input is Zero, it is treated as a padding.\n",
    "        # and a one is written to the result, otherwise a Zero is written to the array (where true -> '1.0': else '0.0')\n",
    "        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "        # Mask has dimensions (batchsize, 1,1, seq_len)\n",
    "        return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "    def create_look_ahead_mask(size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "    def scaled_dot_product_attention(q, k, v, mask):\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "        # scale matmul_qk\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        # add the mask to the scaled tensor.\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "        # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "        # add up to 1.\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "        output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "        def __init__(self, d_model, num_heads):\n",
    "            super(MultiHeadAttention, self).__init__()\n",
    "            self.num_heads = num_heads\n",
    "            self.d_model = d_model\n",
    "\n",
    "            assert d_model % self.num_heads == 0\n",
    "\n",
    "            self.depth = d_model // self.num_heads\n",
    "\n",
    "            self.wq = tf.keras.layers.Dense(d_model)\n",
    "            self.wk = tf.keras.layers.Dense(d_model)\n",
    "            self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "            self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        def split_heads(self, x, batch_size):\n",
    "            \"\"\"Split the last dimension into (num_heads, depth).\n",
    "            Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "            \"\"\"\n",
    "            x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "            return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        def call(self, v, k, q, mask):\n",
    "            batch_size = tf.shape(q)[0]\n",
    "\n",
    "            q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "            k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "            v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "            q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "            k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "            v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "            # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "            # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "            scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "                q, k, v, mask)\n",
    "\n",
    "            scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "            concat_attention = tf.reshape(scaled_attention, \n",
    "                                          (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "            output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "            return output, attention_weights\n",
    "\n",
    "    def point_wise_feed_forward_network(d_model, dff):\n",
    "        return tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "    class EncoderLayer(tf.keras.layers.Layer):\n",
    "        def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "            super(EncoderLayer, self).__init__()\n",
    "\n",
    "            self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "            self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "            self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "            self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "            self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "        def call(self,x,training,mask):\n",
    "            # first sublayer\n",
    "            attn_output,_ = self.mha(x,x,x,mask)\n",
    "            attn_output = self.dropout1(attn_output, training=training)\n",
    "            out1 = self.layernorm1(x+attn_output)\n",
    "\n",
    "            # second sublayer\n",
    "            ffn_output = self.ffn(out1)\n",
    "            ffn_output = self.dropout2(ffn_output, training=training)\n",
    "            out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "            return out2\n",
    "\n",
    "\n",
    "    class DecoderLayer( tf.keras.layers.Layer ):\n",
    "        def __init__(self, d_model, num_heads, dff, rate = 0.1):\n",
    "            super(DecoderLayer, self).__init__()\n",
    "\n",
    "            self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "            self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "            self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "            self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "            self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "            self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "            self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "        def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "            # first sublayer\n",
    "            attn1, attn_weights_block1 = self.mha1(x,x,x,look_ahead_mask)\n",
    "            attn1 = self.dropout1(attn1, training=training)\n",
    "            out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "            # second sublayer\n",
    "            attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "            attn2 = self.dropout2(attn2, training=training)\n",
    "            out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "            # third sublayer\n",
    "            ffn_output = self.ffn(out2)\n",
    "            ffn_output = self.dropout3(ffn_output, training=training)\n",
    "            out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "            return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "    class Encoder(tf.keras.layers.Layer):\n",
    "        def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "            super(Encoder, self).__init__()\n",
    "\n",
    "            self.d_model = d_model\n",
    "            self.num_layers = num_layers\n",
    "\n",
    "            self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "            self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "\n",
    "            self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "            self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "        def call(self, x, training, mask):\n",
    "            seq_len = tf.shape(x)[1]\n",
    "\n",
    "            x = self.embedding(x)\n",
    "            x*= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "            x+= self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "            x= self.dropout(x, training = training)\n",
    "\n",
    "            for i in range(self.num_layers):\n",
    "                x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "            return x\n",
    "\n",
    "    class Decoder(tf.keras.layers.Layer):\n",
    "        def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "            super(Decoder, self).__init__()\n",
    "\n",
    "            self.d_model = d_model\n",
    "            self.num_layers = num_layers\n",
    "\n",
    "            self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "            self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "            self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "            self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "        def call(self,x,enc_output, training, look_ahead_mask, padding_mask):\n",
    "            seq_length= tf.shape(x)[1]\n",
    "            attention_weights = {}\n",
    "\n",
    "            # batchsize, sequence_length, d_model\n",
    "            x  =self.embedding(x)\n",
    "            x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "            x +=self.pos_encoding[:, :seq_length, :]\n",
    "\n",
    "            x= self.dropout(x, training=training)\n",
    "\n",
    "            for i in range(self.num_layers):\n",
    "                x,  block1, block2 = self.dec_layers[i](x,enc_output, training, look_ahead_mask, padding_mask)\n",
    "                attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "                attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "            return x, attention_weights\n",
    "\n",
    "\n",
    "    class Transformer(tf.keras.Model):\n",
    "        def __init__(self, num_layers, d_model, num_heads, dff, \n",
    "                     input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "            super(Transformer, self).__init__()\n",
    "\n",
    "            self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "            self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "\n",
    "            self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "        def call(self, inp, tar, training, \n",
    "                 enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "            # batch_size, inp_seq_length, d_model\n",
    "            enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "            dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "            final_output = self.final_layer(dec_output)\n",
    "\n",
    "            return final_output, attention_weights\n",
    "\n",
    "\n",
    "    num_layers = 3\n",
    "    d_model = 512\n",
    "    dff=1536\n",
    "    num_heads = 8\n",
    "\n",
    "    bpe_encoder_vocab_size = 16272\n",
    "    ## +2 because of padding and masking\n",
    "    input_vocab_size = bpe_encoder_vocab_size + 2\n",
    "    target_vocab_size = bpe_encoder_vocab_size + 2\n",
    "\n",
    "\n",
    "    #input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "    #target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "\n",
    "\n",
    "    dropout_rate = 0.09\n",
    "\n",
    "    class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, d_model, warmup_steps=4000):\n",
    "            super(CustomSchedule, self).__init__()\n",
    "\n",
    "            self.d_model=d_model\n",
    "            self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "            self.warmup_steps = warmup_steps\n",
    "\n",
    "        def __call__(self, step):\n",
    "            arg1 = tf.math.rsqrt(step)\n",
    "            arg2 = step * (self.warmup_steps ** -1.5 )\n",
    "\n",
    "            return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    learning_rate = CustomSchedule(d_model)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "\n",
    "    loss_object=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "    def loss_function(real, pred):\n",
    "        mask=tf.math.logical_not(tf.math.equal(real,0))\n",
    "\n",
    "        loss_ = loss_object(real, pred)\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "\n",
    "        loss_ *= mask\n",
    "        return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
    "\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "    transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size,\n",
    "                         pe_input=input_vocab_size, pe_target=target_vocab_size,\n",
    "                         rate=dropout_rate)\n",
    "\n",
    "\n",
    "    def create_masks(inp, tar):\n",
    "        # encoder padding mask\n",
    "        enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # wird im second attentionblock im decoder benutzt, um den input zu maskieren\n",
    "        dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_target_padding_mask = create_padding_mask(tar)\n",
    "        combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "        return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "\n",
    "    checkpoint_path = \"./checkpoints/train\"\n",
    "    ckpt = tf.train.Checkpoint( transformer = transformer, optimizer = optimizer)\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "    if ckpt_manager.latest_checkpoint:\n",
    "        ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "        print('Latest checkpoint restored!')\n",
    "\n",
    "\n",
    "    EPOCHS = 10\n",
    "\n",
    "    train_step_signature = [\n",
    "        tf.TensorSpec(shape=(None,None), dtype=tf.int64),\n",
    "        tf.TensorSpec(shape=(None,None), dtype=tf.int64)\n",
    "    ]\n",
    "\n",
    "    @tf.function(input_signature = train_step_signature)\n",
    "    def train_step(inp, tar):\n",
    "        tar_inp = tar[:, :-1]\n",
    "        tar_real = tar[:, 1:]\n",
    "\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, _ = transformer(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "            loss = loss_function(tar_real, predictions)\n",
    "\n",
    "            gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "        train_accuracy(tar_real, predictions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "\n",
    "        for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "            train_step(inp,tar)\n",
    "\n",
    "            if batch%100 ==0:\n",
    "                print('Epoch{} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "                    epoch+1, batch, train_loss.result(), train_accuracy.result()\n",
    "                ))\n",
    "\n",
    "        if (epoch+1)%2 == 0:\n",
    "            ckpt_save_path = ckpt_manager.save()\n",
    "            print('saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "\n",
    "        print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch+1, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "        print('Time taken for one epoch: {} secs\\n'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the training using a validation dataset\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [tokenizer_pt.vocab_size]\n",
    "    end_token = [tokenizer_pt.vocab_size + 1]\n",
    "  \n",
    "    # inp sentence is portuguese, hence adding the start and end token\n",
    "    inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "  \n",
    "    # as the target is english, the first word to the transformer should be the\n",
    "    # english start token.\n",
    "    decoder_input = [tokenizer_en.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(MAX_LENGTH):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "  \n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "    \n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == tokenizer_en.vocab_size+1:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    sentence = tokenizer_pt.encode(sentence)\n",
    "\n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "\n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "\n",
    "        ax.set_ylim(len(result)-1.5, -0.5)\n",
    "\n",
    "        ax.set_xticklabels(\n",
    "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "            fontdict=fontdict, rotation=90)\n",
    "\n",
    "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                            if i < tokenizer_en.vocab_size], \n",
    "                           fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence, plot=''):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "    predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                              if i<tokenizer_en.vocab_size])\n",
    "    \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted Translation: {}'.format(predicted_sentence))\n",
    "    \n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence, result, plot)\n",
    "        \n",
    "#translate(\"este é um problema que temos que resolver.\")\n",
    "#print (\"Real translation: this is a problem we have to solve .\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translate(\"este é o primeiro livro que eu fiz.\", plot='decoder_layer6_block2')\n",
    "#print (\"Real translation: this is the first book i've ever done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
