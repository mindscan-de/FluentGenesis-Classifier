{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Prediction using shared embeddings\n",
    "\n",
    "(C) Maxim Gansert, Mindscan, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../../src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of our Target\n",
    "\n",
    "* load the embeddings\n",
    "* instantiate and load a pretrained transformer model\n",
    "* provide multiple interactive boxes\n",
    "  * classname\n",
    "  * method name\n",
    "  * method signature\n",
    "  * context\n",
    "  * current line\n",
    "  \n",
    "* implement an interactive predictor, which can be queried to provide the next tokens\n",
    "* also implement a filter mechanism so that bpe tokens can be filtered to only ones matching the input... so these can be sampled - kind of subword input / subword start - tree search?\n",
    "\n",
    "## Performance improvements can be gained\n",
    "\n",
    "* by caching the masked multihead attention calculations for each layer, for each input (self attention)\n",
    "* by caching the multihead attention calculatuion connected to the encoder K,V\n",
    "\n",
    "## Performance improvements\n",
    "\n",
    "* the model might be also optimized with a prune and quantization model optimizer, so it performs faster, if zeros are found\n",
    "\n",
    "# Support Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from com.github.c2nes.javalang import tokenizer as tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the BPE Encodings and the BPE-Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load our BPE-Model, for encoding all the java tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from de.mindscan.fluentgenesis.bpe.bpe_model import BPEModel\n",
    "from de.mindscan.fluentgenesis.bpe.bpe_encoder_decoder import SimpleBPEEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYMBOL_PAD = 0\n",
    "SYMBOL_START = 16273\n",
    "SYMBOL_EOS = 16274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpemodel = BPEModel(\"16K-full\", \"../../src/de/mindscan/fluentgenesis/bpe/\")\n",
    "bpemodel.load_hparams()\n",
    "bpemodel_vocabulary = bpemodel.load_tokens()\n",
    "bpemodel_bpe_data = bpemodel.load_bpe_pairs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend the vocabulary which was used during the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "bpemodel_vocabulary['<PAD>'] = SYMBOL_PAD\n",
    "# start symbol\n",
    "bpemodel_vocabulary['<START>'] = SYMBOL_START\n",
    "# end of sentence\n",
    "bpemodel_vocabulary['<EOS>'] = SYMBOL_EOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_encoder = SimpleBPEEncoder(bpemodel_vocabulary, bpemodel_bpe_data)\n",
    "\n",
    "MODEL_VOCABULARY_LENGTH = len(bpemodel_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Transformer model using the checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from de.mindscan.fluentgenesis.transformer import TfTransformerV2\n",
    "\n",
    "MAX_OUTPUTLENGTH = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfTransformerV2.Transformer(\n",
    "    num_layers=4, d_model=256, num_heads=8, dff=1024,\n",
    "    input_vocab_size=16275, target_vocab_size=16275,\n",
    "    pe_input=512, pe_target=512,\n",
    "    rate=0.0\n",
    "    )\n",
    "\n",
    "transformer.load_weights(filepath='../../data/checkpoints/nextlineofcode_s_emb/v5/tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    # this will create a mask from the input, whereever the input is Zero, it is treated as a padding.\n",
    "    # and a one is written to the result, otherwise a Zero is written to the array (where true -> '1.0': else '0.0')\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "    # Mask has dimensions (batchsize, 1,1, seq_len)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "    # encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    \n",
    "    # wird im second attentionblock im decoder benutzt, um den input zu maskieren\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "    \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "    \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_transformer_nextline(transformer, input_tokens, output_tokens):\n",
    "    encoderinput = tf.expand_dims(input_tokens,0)\n",
    "    output = tf.expand_dims(output_tokens,0)\n",
    "\n",
    "    for _ in range(MAX_OUTPUTLENGTH):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoderinput,output)\n",
    "        \n",
    "        predictions, attention_weights = transformer(encoderinput,\n",
    "                                                     output, \n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask\n",
    "                                                     )\n",
    "        predictions = predictions[:,-1, :]\n",
    "        # greedy sampling\n",
    "        # predicted_id = tf.argmax(predictions, axis=-1, output_type=tf.int32)\n",
    "        predicted_id = tf.random.categorical(logits=predictions, num_samples=1)[0]\n",
    "        \n",
    "        if predicted_id in(SYMBOL_PAD, SYMBOL_EOS):\n",
    "            return tf.squeeze(output, axis=0), attention_weights, input_tokens\n",
    "        \n",
    "        output = tf.concat( [output, [predicted_id]], axis=-1 )\n",
    "        \n",
    "    return tf.squeeze(output, axis=0), attention_weights, input_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_java_code(theSource: str):\n",
    "    tokens = list(tokenizer.tokenize(theSource, ignore_errors=True))\n",
    "    tokenvalues = [x.value for x in tokens]\n",
    "    \n",
    "    return tokenvalues\n",
    "\n",
    "def predict_line(transformer , class_name, method_name, method_signature, context, current_line):\n",
    "    # build the input token list\n",
    "    input_tokens = []\n",
    "    input_tokens.append( SYMBOL_START )\n",
    "    input_tokens.extend( bpe_encoder.encode( [ class_name, '.', method_name ] ) )\n",
    "    input_tokens.extend( bpe_encoder.encode( tokenize_java_code( method_signature ) ) )\n",
    "    # use the previous lines as context\n",
    "    input_tokens.extend( bpe_encoder.encode( tokenize_java_code( context ) ) )\n",
    "    # complete the input token list with the end of sentence symbol\n",
    "    input_tokens.append( SYMBOL_EOS )\n",
    "    \n",
    "    # the last Java token may not be complete, if space or symbol, then the input is complete\n",
    "    # otherwise we should mark the last tokens as not complete and use the name as a \"preference\".\n",
    "    output_tokens = []\n",
    "    output_tokens.append( SYMBOL_START )\n",
    "    output_tokens.extend( bpe_encoder.encode( tokenize_java_code(current_line ) ) )\n",
    "    \n",
    "    result,_,_ = sample_transformer_nextline(transformer, input_tokens, output_tokens)\n",
    "    result = result.numpy()\n",
    "    \n",
    "    return bpe_encoder.decode(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hLayout = widgets.Layout(width='80%')\n",
    "vLayout = widgets.Layout(width='80%', height='150px')\n",
    "\n",
    "outputTextArea = widgets.Textarea(description='prediction(s):', layout=vLayout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classnameInputTextField = widgets.Text(\n",
    "    value='', \n",
    "    placeholder='class name goes here',\n",
    "    description='String:',\n",
    "    disabled=False, \n",
    "    layout=hLayout\n",
    ")\n",
    "\n",
    "methodnameInputTextField = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='method name goes here',\n",
    "    description='String:',\n",
    "    disabled=False, \n",
    "    layout=hLayout\n",
    ")\n",
    "\n",
    "methodsignatureInputTextField = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='method signature goes here',\n",
    "    description='String:',\n",
    "    disabled=False, \n",
    "    layout=hLayout\n",
    ")\n",
    "\n",
    "methodcontextInputTextArea = widgets.Textarea(\n",
    "    description='Context:',\n",
    "    layout=vLayout\n",
    ")\n",
    "\n",
    "currentLineInputTextField = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='current line',\n",
    "    description='CurrentLine:',\n",
    "    disabled=False,\n",
    "    layout=hLayout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def currentLineHandler(obj):\n",
    "    global transformer\n",
    "    updated_line = obj.new\n",
    "    \n",
    "    # update the context, such\n",
    "    if updated_line.endswith('\\n'):\n",
    "        methodcontextInputTextArea.value = methodcontextInputTextArea.value + updated_line\n",
    "        currentLineInputTextField.value = ''\n",
    "        return\n",
    "    \n",
    "    # encode each thing and run the predictor with the classname, the methodname, the signature,\n",
    "    class_name = classnameInputTextField.value\n",
    "    method_name = methodnameInputTextField.value\n",
    "    method_signature = methodsignatureInputTextField.value\n",
    "    context = methodcontextInputTextArea.value\n",
    "    current_line = currentLineInputTextField.value\n",
    "    \n",
    "    predicted_tokens = predict_line(transformer , class_name, method_name, method_signature, context, current_line)\n",
    "    \n",
    "    outputTextArea.value = ' '.join(predicted_tokens)\n",
    "    \n",
    "currentLineInputTextField.observe(currentLineHandler, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display (classnameInputTextField)\n",
    "display (methodnameInputTextField)\n",
    "display (methodsignatureInputTextField)\n",
    "display (methodcontextInputTextArea)\n",
    "display (currentLineInputTextField)\n",
    "\n",
    "display (outputTextArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
