{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Experiments 0x02\n",
    "----\n",
    "(C) Maxim Gansert, 2020, Mindscan Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils.extmath import softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to experiment with the attention mechanism. This is one of the things i do not understand right now. The following steps shall be achieved:\n",
    "\n",
    "* reuse my learned embeddings **done**\n",
    "* use a fixed vector **done**\n",
    "* do the attention calculation **done**\n",
    "* visualize the attention **done**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load embedding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from de.mindscan.fluentgenesis.embedding.embedder import Embedder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Embedder()\n",
    "embedder.load(\"../../data/16k-full-embeddings/syn0.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will embed a sequence of int32 into a matrix of (len(input) x 512)\n",
    "\n",
    "bpe = [461, 124, 648, 92, 94, 2128, 645, 640, 62, 864, 47, 3357, 41, 5946, 42, 60, 10160, 1712, 62, 10160, 47, 1465, 41, 35, 4151, 10423, 42, 60, 320, 1712, 47, 5438, 41, 2128, 645, 640, 42, 60, 126, 633, 41, 349, 102, 42, 124, 320, 346, 60, 126]\n",
    "E = embedder.embed(bpe)\n",
    "\n",
    "print(E.shape)\n",
    "print(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = E\n",
    "Q = E\n",
    "V = E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.dot(K, Q.T) / math.sqrt(512)\n",
    "print(scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this softmax (sklearn) function work row wise, (line by line), we can see that because \n",
    "# the matrix is not symatric any more.\n",
    "\n",
    "softscores = softmax(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(softscores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import NoNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention( attn ):\n",
    "    fig, ax = plt.subplots(figsize=(8,8) , dpi=150)\n",
    "    im = ax.imshow(attn, cmap=plt.get_cmap('gray'), norm=NoNorm(), interpolation='none')\n",
    "\n",
    "    ax.set_xticks(np.arange(len(bpe)))\n",
    "    ax.set_yticks(np.arange(len(bpe)))\n",
    "\n",
    "    ax.set_xticklabels(bpe)\n",
    "    ax.set_yticklabels(bpe)\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    ax.set_title(\"Attention\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_attention2( attn ):\n",
    "    fig, ax = plt.subplots(figsize=(12,30) , dpi=150)\n",
    "    im = ax.imshow(attn, cmap=plt.get_cmap('gray'))\n",
    "\n",
    "    # ax.set_xticks(np.arange(len(bpe)))\n",
    "    ax.set_yticks(np.arange(len(bpe)))\n",
    "\n",
    "    # ax.set_xticklabels(bpe)\n",
    "    ax.set_yticklabels(bpe)\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    ax.set_title(\"Weighted Embeddings.\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention(softscores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Attention-Mechanism (Pytorch) \n",
    "\n",
    "This code is for reference and is equivalent to the examples in \"the annotated transformer\" which implements the transformder network described in \"Attention is all you need\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "Q=torch.tensor(E, device=device).float()\n",
    "K=torch.tensor(E, device=device).float()\n",
    "V=torch.tensor(E, device=device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_scores = torch.matmul(Q,K.transpose(-2,-1)) / math.sqrt(512)\n",
    "\n",
    "print(t_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_attn = F.softmax(t_scores, dim = -1)\n",
    "\n",
    "print(p_attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we hope to see is that the pytorch implementation has similar results to the numpy implementation above, to be sure that the numpy implementation is doing the same calculations as the pytorch(reference) implementation.\n",
    "\n",
    "Since I don't know pytorch very well, i want to have a consistent view in numpy so i can translate that later to a tensorflow implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "plot_attention(p_attn.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=torch.matmul(p_attn, V)\n",
    "\n",
    "plot_attention2(result.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.cpu().size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Attention-Mechanism (numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value):\n",
    "    d_k = 512\n",
    "    scores = np.dot(query, key.T) / math.sqrt(d_k)\n",
    "    p_attn = softmax(scores)\n",
    "    \n",
    "    return np.dot(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simple_attention(input):\n",
    "    out, pattn =attention(input,input,input)\n",
    "    plot_attention(pattn)\n",
    "    plot_attention2(out)\n",
    "run_simple_attention(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention-Mechanism (numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does multihead attention work? Instead of having one computation of Attention for the whole embedding (of sequence length x embedding dimensions (e.g. 512)) we divide the embeddings into smaller ones, by \"splitting\" the embedding vectors into smaller ones. **(But unfortunately this is not how it is done)**\n",
    "\n",
    "Let's assume we have 16 attention heads, we split a 49x512 embedding into 16 adjacent tiles of size 49x32. If we have 8 attention heads, we split the 49x512 embedding into 8 adjacent tiles of size 49x64. The embeddings used have different statistical properties for every dimension, resulting in different self attention matrices for each tile (they don't look the same. Thus the weighting of the values will be different. You can see the different sttention matrices below.\n",
    "\n",
    "There is still the question open, on how to proceed with those attention matrices and how to combine the different results.\n",
    "We simply can calculate different attentions. But what then?\n",
    "\n",
    "  * use the only on the particular bloc, where this atention is derived from?\n",
    "  * concat these attentions and do some magic with the \"value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://stackoverflow.com/questions/16856788/slice-2d-array-into-smaller-2d-arrays\n",
    "\n",
    "def blockshaped(arr, nrows, ncols):\n",
    "    \"\"\"\n",
    "    Return an array of shape (n, nrows, ncols) where\n",
    "    n * nrows * ncols = arr.size\n",
    "\n",
    "    If arr is a 2D array, the returned array should look like n subblocks with\n",
    "    each subblock preserving the \"physical\" layout of arr.\n",
    "    \"\"\"\n",
    "    h, w = arr.shape\n",
    "    assert h % nrows == 0, \"{} rows is not evenly divisble by {}\".format(h, nrows)\n",
    "    assert w % ncols == 0, \"{} cols is not evenly divisble by {}\".format(w, ncols)\n",
    "    return (arr.reshape(h//nrows, nrows, -1, ncols)\n",
    "               .swapaxes(1,2)\n",
    "               .reshape(-1, nrows, ncols))\n",
    "\n",
    "heads = 8\n",
    "eSplitted = blockshaped(E, 49, 512//heads)\n",
    "print(eSplitted.shape)\n",
    "\n",
    "for i in range(0,heads):\n",
    "    run_simple_attention(eSplitted[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The self attention describes how much a word (line) is connected/related to the i-th word (column) in the sentence (contextual relationship). If we split the attention by splitting the embeddings, it creates attentions across multiple dimensions encoded in the embedding vector. But we also do not care about what each dimension in the vector encodes. But having multiple attentions can help to keep track of multiple ideas/concepts in the given input sentence.\n",
    "**(Sorry but the conclusion is wrong here...)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_simple_attention(input):\n",
    "    results = []\n",
    "    for i in range(0,heads):\n",
    "        out, pattn =attention(input[i],input[i],input[i])\n",
    "        results.append(pattn)\n",
    "    \n",
    "    plot_attention( (results[0]+results[1]+results[2]+results[3]+results[4]+results[5]+results[6]+results[7])/8 )\n",
    "    \n",
    "sum_simple_attention(eSplitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head-Attention -- Part 2\n",
    "The real multi-head attention is implemented by using a learned weighting for V, K and Q. We have three weight matrices for each attention head. which reduces the dimensionality of K and Q to d_k, d_q = d_model // heads. For a model using 512d-embeddings and 8 heads we have d_k=64 and d_q=64 because of 64 = 512 // 8.\n",
    "\n",
    "In the Transformer paper it seems that d_k, d_q and d_v are of different dimensions. In the tensorflow implementation these are equal. But i still have to investigate that further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
