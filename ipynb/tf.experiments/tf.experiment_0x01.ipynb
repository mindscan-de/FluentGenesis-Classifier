{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Experiments 0x01\n",
    "----\n",
    "(C) Maxim Gansert, 2020, Mindscan Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook tries to explore the attention mechanism of the transformer architecture using Tensorflow an or Keras. The idea is to implement a transformer like architecture, which i can better understand, the code for various transformer arch√≠tectures is auther written with a different API like pytorch, or seems to be somehow overcomplicated to me. Solving too many problems in an overly optimized implementation.\n",
    "\n",
    "The code for the given transformer-implementations seem to lack any simplicity, which makes it har to understand the different papers itself.\n",
    "\n",
    "Since I cannot reuse already trained models, i would like to train such a model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras import Input, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a series of notebooks dedicated to experiment with the tensorflow API and tensorflow.keras API. The goal is to be able to implement different kind of onstandard neuronal network architectures. E.g. to implement differnt layer structures or different building blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def attention(query, key, value, mask=None, dropout=None):\n",
    "#    q_dimension = query.\n",
    "#    scores = tf.matmul(query, tf.transpose(key)) / math.sqrt(d_k)\n",
    "#    \n",
    "#    if mask is not None:\n",
    "#        pass\n",
    "#    \n",
    "#    p_attention = tf.softmax(scores, dim=-1)\n",
    "#    \n",
    "#    if dropout is not None:\n",
    "#        p_attention = dropout(p_attention)\n",
    "#        \n",
    "#    return tf.matmul(p_attention, value), p_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = tf.keras.Input(shape=(512,), name = \"Q\")\n",
    "key = tf.keras.Input(shape=(512,), name=\"K\")\n",
    "value = tf.keras.Input(shape=(512,), name=\"V\")\n",
    "\n",
    "k_dim = 512\n",
    "\n",
    "# scores = tf.matmul(query, tf.transpose(key)) / math.sqrt(k_dim)\n",
    "# scores = tf.math.multiply(query, tf.transpose(key)) / math.sqrt(k_dim)\n",
    "scores = tf.math.multiply(query, key) / math.sqrt(k_dim)\n",
    "\n",
    "p_attention = tf.nn.softmax(scores, name=\"Softmax\")\n",
    "\n",
    "foo = tf.math.multiply(p_attention, value, name=\"AOutput\")\n",
    "\n",
    "outputs = foo\n",
    "\n",
    "model = Model(inputs = (query, key, value), outputs = outputs)\n",
    "model.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = tf.keras.Input(shape=(512,), name = \"Q\")\n",
    "key = tf.keras.Input(shape=(512,), name=\"K\")\n",
    "value = tf.keras.Input(shape=(512,), name=\"V\")\n",
    "\n",
    "k_dim = 512\n",
    "\n",
    "# scores = tf.matmul(query, tf.transpose(key)) / math.sqrt(k_dim)\n",
    "# scores = tf.math.multiply(query, tf.transpose(key)) / math.sqrt(k_dim)\n",
    "scores = (query * key) / math.sqrt(k_dim)\n",
    "\n",
    "p_attention = tf.nn.softmax(scores, name=\"Softmax\")\n",
    "\n",
    "foo = tf.math.multiply(p_attention, value, name=\"AOutput\")\n",
    "\n",
    "outputs = foo\n",
    "\n",
    "model = Model(inputs = (query, key, value), outputs = outputs)\n",
    "model.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = tf.keras.Input(shape=(512,), name = \"Q\")\n",
    "key = tf.keras.Input(shape=(512,), name=\"K\")\n",
    "value = tf.keras.Input(shape=(512,), name=\"V\")\n",
    "\n",
    "k_dim = 512\n",
    "\n",
    "# scores = tf.matmul(query, tf.transpose(key)) / math.sqrt(k_dim)\n",
    "scores = tf.math.multiply(query, tf.transpose(key)) / math.sqrt(k_dim)\n",
    "# scores = tf.math.multiply(query, key) / math.sqrt(k_dim)\n",
    "\n",
    "p_attention = tf.nn.softmax(scores, name=\"Softmax\")\n",
    "\n",
    "foo = tf.math.multiply(p_attention, value, name=\"AOutput\")\n",
    "\n",
    "outputs = foo\n",
    "\n",
    "model = Model(inputs = (query, key, value), outputs = outputs)\n",
    "model.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read:\n",
    "* https://arxiv.org/pdf/1810.04805.pdf\n",
    "* https://arxiv.org/pdf/1909.11942.pdf\n",
    "** https://github.com/google-research/albert/blob/master/modeling.py\n",
    "\n",
    "* https://github.com/google-research/bert/blob/master/modeling.py\n",
    "\n",
    "\n",
    "* https://www.tensorflow.org/tutorials/text/word_embeddings\n",
    "* https://www.tensorflow.org/tutorials/text/text_classification_rnn\n",
    "* https://www.tensorflow.org/tutorials/text/text_generation\n",
    "* https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "* https://www.tensorflow.org/tutorials/text/image_captioning\n",
    "* https://www.tensorflow.org/tutorials/text/transformer\n",
    "\n",
    "* https://www.tensorflow.org/tutorials/generative/style_transfer\n",
    "* https://www.tensorflow.org/tutorials/generative/deepdream\n",
    "* https://www.tensorflow.org/tutorials/generative/dcgan\n",
    "* https://www.tensorflow.org/tutorials/generative/pix2pix\n",
    "* https://www.tensorflow.org/tutorials/generative/cyclegan\n",
    "* https://www.tensorflow.org/tutorials/generative/adversarial_fgsm\n",
    "* https://www.tensorflow.org/tutorials/generative/cvae\n",
    "\n",
    "* https://www.tensorflow.org/tutorials\n",
    "\n",
    "* https://www.tensorflow.org/api_docs/python/tf/math/divide\n",
    "* https://github.com/CyberZHG/keras-bert\n",
    "* https://pypi.org/project/keras-bert/\n",
    "* https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b\n",
    "** https://github.com/strongio/keras-bert\n",
    "\n",
    "Dieses scheint gut zu sein\n",
    "* https://machinetalk.org/2019/04/29/create-the-transformer-with-tensorflow-2-0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
