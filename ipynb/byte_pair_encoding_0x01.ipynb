{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding (Tutorial 0x01) - The Algorithm\n",
    "(C) Maxim Gansert, 2019, 2022, Mindscan Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Byte Pair Encoding is a compression scheme, (I found a description of that in an article in the \"C/C++ Users Journal\" of September 1997, Page 23--30, by Philip Gage)\n",
    "\n",
    "Today this algorithm is used for encoding textual inputs, which is preprocessed for NLP/NLU-tasks (natural language processing, natural language understanding) it allows encoding/pairing the characters in a sub-word-level and also provides an option for encoding unknown words.\n",
    "\n",
    "How does it being intended to work?\n",
    "\n",
    "### Encode ###\n",
    "\n",
    "\n",
    "Original Description says:\n",
    "\n",
    "The following pseudocode illustrates the compression process:\n",
    "~~~~\n",
    "Read file into buffer\n",
    "While (compression possible)\n",
    "{\n",
    "    find most frequent byte pair in buffer\n",
    "    \n",
    "    add pair to table and assign it an unused byte\n",
    "    \n",
    "    replace all such pairs with this unused byte\n",
    "}\n",
    "Write pair table and buffer\n",
    "~~~~\n",
    "\n",
    "### Decode ###\n",
    "\n",
    "Original Description says:\n",
    "\n",
    "The following pseudocode illustrates the decompression process:\n",
    "\n",
    "~~~~\n",
    "read and store the pair table in buffer\n",
    "while ( stack not empty or not end of file)\n",
    "{\n",
    "    if (stack empty) read byte\n",
    "    else pop byte from stack\n",
    "    \n",
    "    if(byte is pair code) push pair on stack\n",
    "    else write byte      \n",
    "}\n",
    "~~~~\n",
    "\n",
    "### Process ###\n",
    "\n",
    "After each compression step the data to compress needs a reanalysis. So this is a multipass step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Byte Pair Encoding (Tutorial 0x02) - Source Code Tokenization\n",
    "\n",
    "Actually this Encoding is somehow more universal. Today it is used in Language Models for tokenizing on a subword level.\n",
    "\n",
    "Taking this idea, it allows us to understand source code changes, better and mark differences between two lines not on a character by character level but on a level on meaning, because we can use statistics to figure out which parts probably should be considered together.\n",
    "\n",
    "It allows for a quite good and human oriented tokenization. This is an idea, which tries to calculate the difference between two lines of code. But also understand the difference better, instead of comming up with clever heuristics for the character level. This seems to simplify the diff problem by a lot, and leads to a tokenization based on source code statistics.\n",
    "\n",
    "See the notebooks in the bpe_diff folder. I haven't found any reference to the idea to tokenize a diff and work with these tokens to calculate a better human readable diff from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding (Tutorial 0x03) - Reuse Statistics\n",
    "\n",
    "Als a data compression technique this algorithm is quite slow. The idea is to combine the two most adjacently occuring tokens into a new one. So if the 'e' and 'r' token are occuring very often together, and more often than any other we combine these two into a new token 'er' and now find out the now most occuring combination and might find 'i','n'.\n",
    "\n",
    "The main idea is to keep that statistics in what particular order we combined two tokens. If you do this for a large enough corpus we now know a ruleset in which natural order to combine tokens. We can aim for a dictionary size like 10K tokens, that means we store the 10000 most together occuring tokens, where many of the tokens represent full words. If we reapply the same ruleset, which we calculated across a large corpus of text or source code, the result of the encoding operation canbe speed up, instead of counting token pairs, and then selecting one of them as the most orring one, we just assume we calculated th is statistics and we \"just know better\" instead of searching for most effective solution.\n",
    "\n",
    "This ruleset resuse allows us now to encode two files, with the same ruleset, where the same words basically lead to the same combined token. But this generalized ruleset also contains a lot of knowledge about which characters belong to eah other and which are more unlikely to occur together - which can be understood as a divider for words.\n",
    "\n",
    "This rulreset reuse allows now to tokenize the same way and without the recalculation of statistics, we just spend time to figure out which tokens to combine first, then second, then third.\n",
    "\n",
    "We start with tokens being one single letter and ending combining 'artifact' with 's' (but this depends very much on the corpus used to obtain these statistics). \n",
    "\n",
    "Anyways this compression of the whole corpus leads to aquite useful knowledge about which letters belong together and which don't. This in itself basically is alreadyy a language model on it's own. Because of the now fixed ruleset, we will always encode/transform the same word in the same form and arrive at the same final token for that word. This property makes it useful for another language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
