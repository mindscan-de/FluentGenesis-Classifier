{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Transformer-Paper Exploration - Attention is all you need\n",
    "----\n",
    "\n",
    "Inspired by The Annotated Transformer - https://nlp.seas.harvard.edu/2018/04/03/attention.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer presented in \"Attention is all you need\".\n",
    "\n",
    "This is an annotated version of the paper as a line by line implementation. This implementation is not working (yet). But it helps to work on all those primitives (one at a time) and architectural designs. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My goal is to really understand, how to implement a transformer network architecture for myself. My goal is not to teach others, but genuinely understand and being able to implement a transformer architecture from scratch. I don't want to use other peoples code while not understanding it. I might switch later to a given and working codebase, but for the start, I want to understand how this model is used, trained and extended for my own purposes.\n",
    "\n",
    "I have reasons to not use the available original pretrained transformer models, but to train my own. Because I don't target natural language processing with this implementation. Because of that I will try to implement a much smaller baseline, with only a few million parameters, and if it works out, I will decide how to move on. I might go for a bigger model, maybe i have to train it for money, or use my own hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start with the overall architecture first\n",
    "\n",
    "A encoder-decoder architecture is standard right now, we have an encoder on the left side (gray box) and a decoder on the right side (also gray box).\n",
    "\n",
    "![arxiv_1706_03762_fig1](images/arxiv.1706.03762.fig1.png \"Figure 1 of arxiv 1706.03762\")\n",
    "\n",
    "One inference-step (forward step) of this whole model takes an encode step and after that a decode step. We combine these operations:\n",
    "\n",
    "    decode( encode( inputs ), previous_outputs_shifted_right )\n",
    "\n",
    "We have to define two more operations, `encode` and `decode`. \n",
    "\n",
    "The `encode` operation is a combination of embedding the inputs (left red box) and then run the encoding step on it (left gray box). \n",
    "\n",
    "The `decode` operation is a combination of embedding the previously generated outputs(right red box) and then run the decoding step (right gray box) on it using the additional input of the encoding step (arrow(s) from the left gray box into the right gray box)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(object):\n",
    "    '''\n",
    "    This is a simple implementation of an encoder-decoder architecture. It is not specific to the implementation of the \n",
    "    transformer architecture.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, encoder, decoder, source_embedder, target_embedder, generator):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.source_embed = source_embedder\n",
    "        self.target_embed = target_embedder\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, source, target, source_mask, target_mask):\n",
    "        return self.decode(self.encode(source, source_mask), source_mask, target, target_mask)\n",
    "    \n",
    "    def encode(self, source, source_mask):\n",
    "        return self.encoder(self.source_embed(source), src_mask)\n",
    "    \n",
    "    def decode(self, memory, source_mask, target, target_mask):\n",
    "        return self.decoder(self.target_embed(target), memory, source_mask, target_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After modeling the red and gray boxes, we need to consider the light blue `Linear` layer and green `Softmax` layer. We usually call this the projection layer. Because it does the transformation of the last layer of the output to a kind of \"orthogonal\" representation of the output which selects one output out of many. We make a projection of the output into n one-dimensional outputs. The number of outputs is the same number as words/tokens in the output dictionary.\n",
    "\n",
    "Because the projection layer also generates the next word/token, it is also called the `Generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(object):\n",
    "    '''\n",
    "    This is a standard projection layer, used for generating the next word in a standard linear and softmax layout.\n",
    "    '''\n",
    "    def __init_(self, output_embedding_dimensions, vocabsize ):\n",
    "        # linear nodes, has input of output_embedding words and the outputsize of vocabsize\n",
    "        self.projection = linear(embedding_dimensions, vocabsize)\n",
    "        \n",
    "    def forward(self, x ):\n",
    "        return F.log_softmax(self.projection(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder and Decoder Stacks\n",
    "\n",
    "### Encoder\n",
    "Let's have a look at the encoder again and work through it.\n",
    "\n",
    "![arxiv_1706_03762_fig1.zoomencoder](images/arxiv.1706.03762.fig1.zoomencoder.png \"Figure 1 of arxiv 1706.03762 Encoder zoomed in\")\n",
    "\n",
    "It turned out, that the number six is quite a good choice for language understanding / Languae Modelling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_layer(module, N):\n",
    "    '''\n",
    "    This module will clone a module and produce N identical layers\n",
    "    '''\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wonder, why there is a normalization as the last element of the Encoder... but let's accept that for now. But i guess, there is more usage of the pattern more than once...\n",
    "\n",
    "The final Layer Normalization is applied, because the following implementation. The following implementation decides to apply the normalization before the layer is applied. This might be a slightly different network, than that in the paper... because x is not normalized later, which might lead to loss in X over layers. Maybe this is why it is done this way..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(object):\n",
    "    '''\n",
    "    The Encoder is a a stack of N identical layers\n",
    "    '''\n",
    "    \n",
    "    def __init(self, singlelayer, N):\n",
    "        self.layers = clone_layer(singlelayer, N)\n",
    "        self.norm = LayerNorm( singlelayer.size )\n",
    "        \n",
    "    def forward( self, x, mask ):\n",
    "        '''Pass the input and mask through each layer and do a final normalization'''\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "            \n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(object):\n",
    "    \n",
    "    def __init__(self, number_of_features, epsilon = 1e-6 ):\n",
    "        self.a_2 = nn.Parameter(ones(number_of_features))\n",
    "        self.b_2 = nn.parameter(zeros(number_of_features))\n",
    "        self.eps = epsilon\n",
    "        \n",
    "    def forward( self, x):\n",
    "        mean = x.mean(-1, keep_dim=True)\n",
    "        std = x.std(-1, keep_dim=True)\n",
    "        return self.a_2 * (x-mean) / (std+self.eps)  + self.b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer contains two residual connections, with an add and normalization including a dropout (yellow boxes). The dropout is applied either on the blue box or the orange box. Dropout is required for training and is disabled while inferencing. Dropout makes a neural network more reliable and resilient, to always consider multiple inputs, and strong connections will not preferred/or occur, because the stzrong connections may be lost while training and the model should also perform well, and not only rely on the one strong connection only, when it can use other intputs too.\n",
    "\n",
    "Okay lets explain this... This is the yelow box and the left connection of the yellow boxes, where `Feed Forward`(blue box) and `Multi-Head Attention`(orange box) is the given sublayer. This component is executing the whole thing. The normalization is taken over from the previous operation Layer.\n",
    "\n",
    "X is the black line on the left around each sublayer compinent and the input below each sublayer component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubLayerConnection(object):\n",
    "    '''\n",
    "    A residual connection followed by a layer normalization\n",
    "    '''\n",
    "    def __init__(self, size, dropout):\n",
    "        self.norm = LayerNorm( size );\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer ):\n",
    "        '''apply the layer on the normalized input of the previous layer or operation'''\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer consista of two sublayers. One is called the Feed-forward layer and the second sublayer is a multi-head-self attention mechanism.\n",
    "\n",
    "We will both incorporate in one encoder layer. But at first we will not investigate further how they are constructed. The goal is to implement the overall structure first and later care about the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(object):\n",
    "    '''\n",
    "    The Encoder consists of an attention mechanism and a feed forward network\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, size, self_attention, feed_forward, dropout):\n",
    "        self.self_attn = self_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clone_layer(SubLayerConnection(size, dropout),2)\n",
    "        self.size = size\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # apply the self attention mechanism and the residual connection\n",
    "        x = self.sublayer[0](x, lamba x: self.self_attn(x,x,x,mask))\n",
    "        # apply the feed forward network and the residual connection\n",
    "        x = self.sublayer[1](x, self.feed_forward)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "![arxiv_1706_03762_fig1.zoomdecoder](images/arxiv.1706.03762.fig1.zoomdecoder.png \"Figure 1 of arxiv 1706.03762 Decoder zoomed in\")\n",
    "\n",
    "\n",
    "The decoder consists also of six identical layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(object):\n",
    "    def __init__(self, layer, N):\n",
    "        self.layers = clones( layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(object):\n",
    "    '''\n",
    "    One Decoding Layer consists of \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, size, self_attention, source_attention, feed_forward, dropout):\n",
    "        self.self_attn = self_attention\n",
    "        self.source_attention = source_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SubLayerConnection(size, dropout))\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x,x,x,tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.source_attention(x, m, m, src_mask))\n",
    "        x = self.sublayer[2](x, self.feed_forward)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size,size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape),k=1).astype('uint8')\n",
    "    return subsequent_mask==0\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
