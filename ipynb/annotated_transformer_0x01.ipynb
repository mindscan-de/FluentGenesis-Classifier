{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Transformer-Paper Exploration - Attention is all you need\n",
    "----\n",
    "\n",
    "Inspired by The Annotated Transformer - https://nlp.seas.harvard.edu/2018/04/03/attention.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer presented in \"Attention is all you need\".\n",
    "\n",
    "This is an annotated version of the paper as a line by line implementation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My goal is to really understand, how to implement a transformer network architecture for myself. My goal is not to teach others, but genuinely understand and being able to implement a transformer architecture from scratch. I don't want to use other peoples code while not understanding it. I might switch later to a given and working codebase, but for the start, I want to understand how this model is used, trained and extended for my own purposes.\n",
    "\n",
    "I have reasons to not use the available original pretrained transformer models, but to train my own. Because I don't target natural language processing with this implementation. Because of that I will try to implement a much smaller baseline, with only a few million parameters, and if it works out, I will decide how to move on. I might go for a bigger model, maybe i have to train it for money, or use my own hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start with the overall architecture first\n",
    "\n",
    "A encoder-decoder architecture is standard right now, we have an encoder on the left side (gray box) and a decoder on the right side (also gray box).\n",
    "\n",
    "![arxiv_1706_03762_fig1](images/arxiv.1706.03762.fig1.png \"Figure 1 of arxiv 1706.03762\")\n",
    "\n",
    "One inference-step (forward step) of this whole model takes an encode step and after that a decode step. We combine these operations:\n",
    "\n",
    "    decode( encode( inputs ), previous_outputs_shifted_right )\n",
    "\n",
    "We have to define two more operations, `encode` and `decode`. \n",
    "\n",
    "The `encode` operation is a combination of embedding the inputs (left red box) and then run the encoding step on it (left gray box). \n",
    "\n",
    "The `decode` operation is a combination of embedding the previously generated outputs(right red box) and then run the decoding step (right gray box) on it using the additional input of the encoding step (arrow(s) from the left gray box into the right gray box)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(object):\n",
    "    '''\n",
    "    This is a simple implementation of an encoder-decoder architecture. It is not specific to the implementation of the \n",
    "    transformer architecture.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, encoder, decoder, source_embedder, target_embedder, generator):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.source_embed = source_embedder\n",
    "        self.target_embed = target_embedder\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, source, target, source_mask, target_mask):\n",
    "        return self.decode(self.encode(source, source_mask), source_mask, target, target_mask)\n",
    "    \n",
    "    def encode(self, source, source_mask):\n",
    "        return self.encoder(self.source_embed(source), src_mask)\n",
    "    \n",
    "    def decode(self, memory, source_mask, target, target_mask):\n",
    "        return self.decoder(self.target_embed(target), memory, source_mask, target_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
