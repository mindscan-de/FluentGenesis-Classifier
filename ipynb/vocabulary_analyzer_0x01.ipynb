{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE vocabulary analysis\n",
    "\n",
    "(C) Maxim Gansert, 2019, Mindscan\n",
    "\n",
    "Recently I build some bpe-vocabularies, which had way too much tokens because of arabic and asian languages. This notebook is intended to review these vocabularies...\n",
    "\n",
    "Current state: the vocabulary was cleaned up... So the initial reason for this notebook is no longer given.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools \n",
    "from de.mindscan.fluentgenesis.bpe.bpe_model import BPEModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BPEModel(\"16K-full\", \"../src/de/mindscan/fluentgenesis/bpe/\")\n",
    "model.load_hparams()\n",
    "model_vocabulary = model.load_tokens()\n",
    "model_bpe_data = model.load_bpe_pairs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vocabulary_statistics_length = {}\n",
    "\n",
    "for token, _ in model_vocabulary.items():\n",
    "    lengthOfToken = len(token)\n",
    "    if lengthOfToken not in model_vocabulary_statistics_length:\n",
    "        model_vocabulary_statistics_length[lengthOfToken] = 1\n",
    "    else:\n",
    "        model_vocabulary_statistics_length[lengthOfToken] += 1\n",
    "        \n",
    "print(model_vocabulary_statistics_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is something like this:\n",
    "\n",
    "{1: 24804, 2: 1462, 3: 2465, 4: 2213, 5: 1906, 6: 1639, 7: 1426, 8: 1255, 9: 971, 10: 738, 11: 505, 12: 349, 13: 277, 14: 202, 15: 156, 16: 107, 17: 84, 18: 64, 19: 44, 20: 35, 21: 22, 22: 21, 23: 8, 24: 9, 25: 13, 26: 7, 27: 3, 28: 3, 29: 2, 30: 4, 32: 2, 33: 2, 34: 1, 36: 1, 37: 1, 42: 1, 43: 1, 51: 1}\n",
    "\n",
    "The intended dictionary size was about 16K tokens...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functools.reduce( lambda sum,y: sum + model_vocabulary_statistics_length[y] ,model_vocabulary_statistics_length.keys() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result for all tokenlength longer than one char is 16001 (oops, off by one in token calculation...) we can see, that we have another 24804 Tokens of length one. Since we might can not encode every words with pairs, and to takle the unknown word problem, we have to emit all unpaired tokens at the end of the process. There are nearly twice the number of unpaired tokens than paired tokens...\n",
    "\n",
    "These originate from strings containing other languages, so lets identify these\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "unsupported_vocab_ranges = [\n",
    "        # Latin Extended\n",
    "        (0x0100, 0x017F),  # Latin Extended-A\n",
    "        (0x0180, 0x024F),  # Latin Extended-B\n",
    "        (0x1E00, 0x1EFF),  # Latin Extended Additional\n",
    "        (0x2C60, 0x2C7F),  # Latin Extended-C\n",
    "        (0xA720, 0xA7FF),  # Latin Extended-D\n",
    "        (0xAB30, 0xAB6F),  # Latin Extended-E\n",
    "    \n",
    "        # Diacritical\n",
    "        (0x0300, 0x036F),  # Combining Diacritical Marks\n",
    "        (0x1AB0, 0x1AFF),  # Combining Diacritical Marks Extended\n",
    "        (0x1DC0, 0x1DFF),  # Combining Diacritical Marks Supplement\n",
    "        (0x20D0, 0x20FF),  # Combining Diacritical Marks for Symbols\n",
    "    \n",
    "        # IPA & phonetic Extemsion\n",
    "        (0x0250, 0x02AF),  # IPA Extensions\n",
    "        (0x1D00, 0x1D7F),  # Phonetic Extensions\n",
    "        (0x1D80, 0x1DBF),  # Phonetic Extensions Supplement\n",
    "    \n",
    "        # Spacing Modifier Letters\n",
    "        (0x02B0, 0x02FF),  # Spacing Modifier Letters\n",
    "    \n",
    "        # Greek Coptic\n",
    "        (0x0370, 0x03FF),  # Greek and coptic\n",
    "        (0x1F00, 0x1FFF),  # Greek Extended\n",
    "        (0x2C80, 0x2CFF),  # Coptic\n",
    "    \n",
    "        # Cyrillic\n",
    "        (0x0400, 0x04FF),  # Cyrillic)\n",
    "        (0x0500, 0x052F),  # Cyrillic Supplement\n",
    "        (0x2DE0, 0x2DFF),  # Cyrillic Extended-A\n",
    "        (0xA640, 0xA69F),  # Cyrillic Extended-B\n",
    "        (0x1C80, 0x1C8F),  # Cyrillic Extended-C\n",
    "    \n",
    "        # Armenian\n",
    "        (0x0530, 0x058F),  # Armenian \n",
    "    \n",
    "        # Hebrew\n",
    "        (0x0590, 0x05FF),  # Hebrew\n",
    "    \n",
    "        # Arabic\n",
    "        (0x0600, 0x06FF),  # Arabic\n",
    "        (0x0750, 0x077F),  # Arabic Supplement\n",
    "        (0x08A0, 0x08FF),  # Arabic Extended-A\n",
    "        (0xFB50, 0xFDFF),  # Arabic Presentation Forms A\n",
    "        (0xFE70, 0xFEFF),  # Arabic Presentation Forms B\n",
    "    \n",
    "        # Syriac\n",
    "        (0x0700, 0x074F),  # Syriac\n",
    "        (0x0860, 0x086F),  # Syriac Supplement\n",
    "    \n",
    "        # Thaana\n",
    "        (0x0780, 0x07BF),  # Thaana\n",
    "    \n",
    "        # NKo\n",
    "        (0x07C0, 0x07FF),  # NKo\n",
    "    \n",
    "        # Samritan\n",
    "        (0x0800, 0x083F),  # Samaritan\n",
    "    \n",
    "        # Mandaic\n",
    "        (0x0840, 0x085F),  # Mandaic\n",
    "    \n",
    "        # Invalid range\n",
    "        (0x0870, 0x089F),  # Invalid range\n",
    "\n",
    "        # Indian Subkontinent Languages\n",
    "        (0x0900, 0x097F),  # Devanagari\n",
    "        (0xA8E0, 0xA8FF),  # Devanagari Extended\n",
    "         \n",
    "        # (0x0980, 0x09FF), # Bengali\n",
    "        # ...\n",
    "        (0x0900, 0x0DFF),  # India - covers multiple languages\n",
    "        (0xA830, 0xA83F),  # Common Indic Number Forms\n",
    "\n",
    "\n",
    "        (0x0E00, 0x0E7F),  # Thai\n",
    "        (0x0E80, 0x0EFF),  # Lao\n",
    "    \n",
    "        (0x0F00, 0x0FFF),  # Tibetan\n",
    "    \n",
    "        # Myanmar\n",
    "        (0x1000, 0x109F),  # Myanmar\n",
    "        (0xAA60, 0xAA7F),  # Myanmar Extended-A\n",
    "        (0xA9E0, 0xA9FF),  # Myanmar Extended-B\n",
    "    \n",
    "        # Georgian\n",
    "        (0x10A0, 0x10FF),  # Georgian\n",
    "        (0x2D00, 0x2D2F),  # Georgian Supplement\n",
    "        (0x1C90, 0x1CBF),  # Georgian Extended\n",
    "    \n",
    "        # Korean\n",
    "        (0x1100, 0x11FF),  # Hangul Jamo    \n",
    "        (0x3130, 0x318F),  # Hangul Compatibility Jamo    \n",
    "        (0xAC00, 0xD7AF),  # Hangul Syllables\n",
    "        (0xA960, 0xA97F),  # Hangul Jamo Extended-A\n",
    "        (0xD7B0, 0xD7FF),  # Hangul Jamo Extended B\n",
    "    \n",
    "        # Ethiopic\n",
    "        (0x1200, 0x139f), # Ethiopic, Ethopic Supplement\n",
    "        (0xAB00, 0xAB2F), # Ethiopic Extended-A\n",
    "        (0x2D80, 0x2DDF), # Ethiopic Extended\n",
    "\n",
    "        # Cherokee\n",
    "        (0x13A0, 0x13FF),  # Cherokee\n",
    "        (0xAB70, 0xABBF),  # Cherokee Supplement\n",
    "    \n",
    "        # Canadian Aboriginal\n",
    "        (0x1400, 0x167F),  # Unified Canadian Aboriginal Syllabics\n",
    "        (0x18B0, 0x18FF),  # Unified Canadian Aboriginal Syllabics Extended\n",
    "    \n",
    "    \n",
    "        (0x1680, 0x169F),  # Ogham\n",
    "        (0x16A0, 0x16FF),  # Runic\n",
    "        (0x1700, 0x171F),  # Tagalog\n",
    "        (0x1720, 0x173F),  # Hanunoo\n",
    "        (0x1740, 0x175F),  # Buhid\n",
    "        (0x1760, 0x177F),  # Tagbanwa\n",
    "        \n",
    "        # Khmer\n",
    "        (0x1780, 0x17FF),  # Khmer\n",
    "        (0x19E0, 0x19FF),  # Khmer Symbols\n",
    "    \n",
    "        # Mongolian\n",
    "        (0x1800, 0x18AF),  # Mongolian\n",
    "\n",
    "        (0x1900, 0x194F),  # Limbu\n",
    "        (0x1950, 0x197F),  # Tai Le\n",
    "        (0x1980, 0x19DF),  # New Tai Lue\n",
    "    \n",
    "        (0x1A00, 0x1A1F),  # Buginese\n",
    "        (0x1A20, 0x1AAF),  # Tai Tham\n",
    "    \n",
    "        (0x1B00, 0x1B7F),  # Balinese\n",
    "    \n",
    "        (0x1B80, 0x1BBF),  # Sundanese\n",
    "        (0x1CC0, 0x1CCF),  # Sundanese Supplement\n",
    "    \n",
    "        (0x1BC0, 0x1BFF),  # Batak\n",
    "    \n",
    "        (0x1C00, 0x1C4F),  # Lepcha\n",
    "        (0x1C50, 0x1C7F),  # Ol Chiki\n",
    "        (0x1CD0, 0x1CFF),  # Vedic Extensions\n",
    "    \n",
    "        # Punctuation\n",
    "        (0x2000, 0x206F),  # General Punctuation\n",
    "        (0x2E00, 0x2E7F),  # Supplemental Punctuation\n",
    "        (0x3000, 0x303F),  # CJK Symbols and Punctuation\n",
    "        \n",
    "        (0x2070, 0x209F),  # Superscripts and Subscripts\n",
    "        (0x20A0, 0x20CF),  # Currency Symbols\n",
    "    \n",
    "    \n",
    "        # Symbols\n",
    "        (0x2100, 0x26ff), # Letterlike Symbols, ... Miscelaneous Symbols\n",
    "        (0x2700, 0x27FF), # Dingbats & co\n",
    "        (0x2800, 0x28FF), # Braille\n",
    "        (0x2900, 0x2BFF), # Symbols Arrows math\n",
    "    \n",
    "        (0x2D30, 0x2D7F),  # Tifinagh\n",
    "        \n",
    "        (0x2f00, 0x2FFF),  # Kangxi radicals, Ideographic Description Characters\n",
    "    \n",
    "\n",
    "        # CJK\n",
    "        (0x3000, 0xa4FF),\n",
    "        (0xFE30, 0xFE4F),  # CJK Compatibility Forms\n",
    "        (0xF900, 0xFAFF),  # CJK Compatibility Ideographs\n",
    "        (0x2E80, 0x2EFF),  # CJK Radicals Supplement\n",
    "    \n",
    "\n",
    "        #\n",
    "        (0xA500, 0xA63F),  # Vai\n",
    "        (0xA6A0, 0xA6FF),  # Bamum\n",
    "        (0xA700, 0xA71F),  # Modifier Tone Letters\n",
    "        (0xA800, 0xA82F),  # Syloti Nagri\n",
    "        (0xA840, 0xA87F),  # Phags-pa\n",
    "        (0xA880, 0xA8DF),  # Saurashtra\n",
    "        (0xA900, 0xA92F),  # Kayah Li\n",
    "        (0xA930, 0xA95F),  # Rejang\n",
    "        (0xA980, 0xA9DF),  # Javanese\n",
    "    \n",
    "        (0xAA00, 0xAA5F),  # Cham\n",
    "        (0xAA80, 0xAADF),  # Tai Viet\n",
    "    \n",
    "        (0xABC0, 0xABFF),  # Meetei Mayek\n",
    "        (0xAAE0, 0xAAFF),  # Meetei Mayek Extensions\n",
    "\n",
    "        # Private Area\n",
    "        (0xE000,0xF8FF),  # Private Use Area\n",
    "        (0xD800, 0xDB7F), # High Surrogates\n",
    "        (0xDB80, 0xDBFF), # High Private Use Surrogates\n",
    "        (0xDC00, 0xDFFF), # Low Surrogates\n",
    "        # ???\n",
    "        (0x2c00, 0x2c5f), # Glagolitic\n",
    "        \n",
    "    \n",
    "        (0xFB00, 0xFB4F),  # Alphabetic Presentation Forms\n",
    "        (0xFE00, 0xFE0F),  # Variation Selectors\n",
    "        (0xFE10, 0xFE1F),  # Vertical Forms\n",
    "        (0xFE20, 0xFE2F),  # Combining Half Marks\n",
    "        (0xFE50, 0xFE6F),  # Small Form Variants\n",
    "        \n",
    "        (0xFF00, 0xFFEF),  # Halfwidth and Fullwidth Forms\n",
    "    \n",
    "        # OLD and OLDER\n",
    "        (0x010000, 0x10FFFF) # Basically everything what is not as important to be in first ~65000 Codes\n",
    "\n",
    "    ]\n",
    "\n",
    "def is_unsupported_character(char):\n",
    "    char=ord(char)\n",
    "    for bottom, top in unsupported_vocab_ranges:\n",
    "        if char >= bottom and char <= top:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "one_char_elements = filter(lambda x: len(x)==1,model_vocabulary )\n",
    "chars = filter(lambda char: not(is_unsupported_character(char)), one_char_elements)\n",
    "charsasArray = [x for x in chars]\n",
    "\n",
    "orderedChars = sorted(charsasArray, key=lambda item: item)\n",
    "print(len(orderedChars))\n",
    "print(orderedChars)\n",
    "\n",
    "print([\"0x%x\"%ord(item) for item in orderedChars])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
