{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and Deduplicate the method Dataset (0x01)\n",
    "\n",
    "(C) Maxim Gansert, Mindscan Engineering, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will dedupliacte and split the methodNextLineDataset. The criteria for deduplication is the same methodbody and the same method name, while ignoring the method signature. The second criteria is the one for splitting; The method dataset is split using the token length of the method body as an criteria. (12 * 2^k) 0<=k<=9\n",
    "\n",
    "Method being equal to each other must have the sam elength to be compared against each other, so splitting the methods by length before deduplicating them does no harm and speeds up the deduplication process.\n",
    "\n",
    "12 was chosen instead of 16 because 16 produced a too big initial dataset to start with.\n",
    "\n",
    "Why having short methods, the idea is to create a dataset which can be easily trained, e.g short datasets. Some Ideas are as short as 12 tokens, some more complex algorithms require 24 tokens and so on. Proof of concept will be trained on shortest dataset first. Method body -> method name this will reduce the amount of compute and then the embeddings can be reduced to train the larger datasets, refining the embeddings, and so on. Since the training of a transformer network is an computationally intensive method, i will start with short methods first, and then extend the input window size, to not waste compute time.\n",
    "\n",
    "Later we also need to get rid of the same implementation leading to a different method name. But this is not our first priority right now. maybe e wan do this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = 'D:\\\\Downloads\\\\Big-Code-full'\n",
    "dataset_full_filename = os.path.join(dataset_directory, 'methodNextLineDataset.jsonl')\n",
    "\n",
    "output_filename_template = 'dedup_methodNextLineDataset_{}_{}.jsonl'\n",
    "\n",
    "# ranges where to split - two neighboring define a range of \n",
    "ranges=[0,12,24,48,96,192,384,768,1536,3072,6144]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_COLUMN_FILENAME = 'file_name'\n",
    "DATASET_COLUMN_CLASSNAME = 'class_name'\n",
    "\n",
    "DATASET_COLUMN_METHODNAME = 'method_name'\n",
    "DATASET_COLUMN_ENCODED_METHODNAME_LENGTH = 'length_encoded_method_name'\n",
    "DATASET_COLUMN_ENCODED_METHODNAME = 'encoded_method_name'\n",
    "\n",
    "DATASET_COLUMN_ENCODED_METHODBODY_LENGTH = 'length_encoded_method_body'\n",
    "DATASET_COLUMN_ENCODED_METHODBODY = 'encoded_method_body'\n",
    "\n",
    "DATASET_COLUMN_ENCODED_METHODSIGN_LENGTH = 'length_encoded_method_sign'\n",
    "DATASET_COLUMN_ENCODED_METHODSIGN = 'encoded_method_sign'\n",
    "\n",
    "DATASET_COLUMN_ENCODED_METHODBODY_ASTUPLE = 'encoded_method_body_tuple' #  TUPLE - TYPE is comparable in pandas\n",
    "DATASET_COLUMN_ENCODED_METHODNAME_ASTUPLE = 'encoded_method_name_tuple'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head(filename, count=1):\n",
    "    with open(dataset_full_filename) as myfile:\n",
    "        head = [next(myfile) for x in range(count)]\n",
    "    return head\n",
    "\n",
    "print(head(dataset_full_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* first filter by length ( length_encoded_method_body )\n",
    "* then deduplicate by method_name, length_encoded_method_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_deduplicate_methods(fullfilepath, therange):\n",
    "    result = None\n",
    "    # each line contains one json document\n",
    "    chunksize = 240000\n",
    "    count = 0\n",
    "    \n",
    "    for df in pd.read_json(fullfilepath, lines=True, chunksize=chunksize ):\n",
    "        count = count + 1\n",
    "        print ('input dataframe size: {} {} processed'.format(df.shape, count*chunksize))\n",
    "        \n",
    "        df=df[df.length_encoded_method_body.between(therange[0],therange[1])]\n",
    "        print ('methods in range: {}'.format(df.shape))\n",
    "        \n",
    "        # add a hashable version of the method body\n",
    "        df[DATASET_COLUMN_ENCODED_METHODBODY_ASTUPLE] = df.encoded_method_body.apply(lambda x: tuple([item for sublist in x for item in sublist]))\n",
    "        df[DATASET_COLUMN_ENCODED_METHODNAME_ASTUPLE] = df.encoded_method_name.apply(lambda x: tuple(x))\n",
    "        \n",
    "        df = df.drop(columns=[DATASET_COLUMN_FILENAME, DATASET_COLUMN_CLASSNAME,\n",
    "                 DATASET_COLUMN_ENCODED_METHODSIGN_LENGTH, DATASET_COLUMN_ENCODED_METHODSIGN])\n",
    "\n",
    "        # reduce dataset by removing duplicates\n",
    "        reduced = df.drop_duplicates( subset=[DATASET_COLUMN_METHODNAME,\n",
    "                                              DATASET_COLUMN_ENCODED_METHODBODY_LENGTH,\n",
    "                                              DATASET_COLUMN_ENCODED_METHODBODY_ASTUPLE], keep='first', inplace=False)\n",
    "        reduced.reset_index(inplace=True)\n",
    "        \n",
    "        if result is None:\n",
    "            result = pd.DataFrame(columns=reduced.columns)\n",
    "\n",
    "        print('deduplicated dataframe size: {}'.format(reduced.shape))\n",
    "\n",
    "        result = result.append(reduced)\n",
    "        result = result.drop_duplicates( subset=[DATASET_COLUMN_METHODNAME,\n",
    "                                                 DATASET_COLUMN_ENCODED_METHODNAME_LENGTH,\n",
    "                                              DATASET_COLUMN_ENCODED_METHODBODY_ASTUPLE], keep='first', inplace=False )\n",
    "\n",
    "        print('result_dataframe size: {}\\n\\n'.format(result.shape))\n",
    "        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def extract_ranges(ranges, output_filename_template):\n",
    "    for i in range(len(ranges)-1):\n",
    "        start_of_range = ranges[0+i] + 1\n",
    "        end_of_range = ranges[1+i]\n",
    "        \n",
    "        deduplicated = split_and_deduplicate_methods( dataset_full_filename, [ start_of_range, end_of_range ] )\n",
    "        deduplicated = deduplicated.drop(columns=[DATASET_COLUMN_ENCODED_METHODBODY_ASTUPLE, DATASET_COLUMN_ENCODED_METHODNAME_ASTUPLE])\n",
    "\n",
    "        outfile=os.path.join(dataset_directory, output_filename_template.format(start_of_range,end_of_range))\n",
    "        deduplicated.to_json(outfile,orient='records',lines=True)\n",
    "    pass\n",
    "\n",
    "\n",
    "extract_ranges( ranges=ranges, output_filename_template=output_filename_template )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
